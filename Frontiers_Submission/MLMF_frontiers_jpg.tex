%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% This is just an example/guide for you to refer to when submitting manuscripts to Frontiers, it is not mandatory to use Frontiers .cls files nor frontiers.tex  %
% This will only generate the Manuscript, the final article will be typeset by Frontiers after acceptance.   
%                                              %
%                                                                                                                                                         %
% When submitting your files, remember to upload this *tex file, the pdf generated with it, the *bib file (if bibliography is not within the *tex) and all the figures.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass{frontiersSCNS} % for Science, Engineering and Humanities and Social Sciences articles
%\documentclass{frontiersHLTH} % for Health articles
%\documentclass{frontiersFPHY} % for Physics and Applied Mathematics and Statistics articles

%\setcitestyle{square}
\usepackage{url,hyperref,lineno,microtype,subcaption}
\usepackage{color}
\usepackage[onehalfspacing]{setspace}
\linenumbers
\usepackage{tikz} % drawing tools in latex
\usepackage{amsmath,amssymb}
\usepackage{cancel}
\usepackage{xcolor}
\usepackage{siunitx}

\usepackage[ruled,vlined,linesnumbered,boxed]{algorithm2e}
\DeclareMathOperator{\BigO}{\mathcal{O}}

\newcommand*{\mathcolor}{}
\def\mathcolor#1#{\mathcoloraux{#1}}
\newcommand*{\mathcoloraux}[3]{%
  \protect\leavevmode
  \begingroup
    \color#1{#2}#3%
  \endgroup
}


\let\oldnl\nl% Store \nl in \oldnl
\newcommand{\nonl}{\renewcommand{\nl}{\let\nl\oldnl}}% Remove line number for one line
\newcommand\Ccancel[2][black]{\renewcommand\CancelColor{\color{#1}}\cancel{#2}}


\newcommand{\ThAs}{\boldsymbol{\theta}^*_a}
\newcommand{\ThOs}{\boldsymbol{\theta}^*_o}

\newcommand{\ThA}{\boldsymbol{\theta}_a}
\newcommand{\ThO}{\boldsymbol{\theta}_o}
\newcommand{\BAlph}{\boldsymbol{\alpha}_{\mathbf{0:t}}}
\newcommand{\Alpha}{\alpha_{0:t}}
\newcommand{\BPsi}{\boldsymbol{\Psi_{0:t}}}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\dependent}{\rotatebox[origin=c]{180}{$\independent$}}
% Leave a blank line between paragraphs instead of using \\
\definecolor{dark-green}{RGB}{0,100,0}
\definecolor{dark-blue}{RGB}{28,107,145}


\def\keyFont{\fontsize{8}{11}\helveticabold }
\def\firstAuthorLast{Guillaume de Chambrier {et~al.}} %use et al only if is more than 1 author
\def\Authors{Guillaume de Chambrier\,$^{1,*}$, Aude Billard\,$^{1}$}
% Affiliations should be keyed to the author's name with superscript numbers and be listed as follows: Laboratory, Institute, Department, Organization, City, State abbreviation (USA, Canada, Australia), and Country (without detailed address information such as city zip codes or street names).
% If one of the authors has a change of address, list the new address below the correspondence details using a superscript symbol and use the same symbol to indicate the author in the author list.
\def\Address{$^{1}$\'Ecole Polyt\'echnique F\'ed\'erale de Lausanne (EPFL), Route Cantonale, 1015 Lausanne, Switzerland}
% The Corresponding Author should be marked with an asterisk
% Provide the exact contact address (this time including street name and city zip code) and email of the corresponding author
\def\corrAuthor{Guillaume de Chambrier}
\def\corrEmail{guillaume.dechambrier@epfl.ch}

\tikzstyle{white_box}  = [draw=black, fill=white, very thick,  rectangle, rounded corners, inner sep=10pt, inner ysep=20pt]
\tikzstyle{fancytitle} = [fill=white,draw= black, text=white,rounded corners=1mm,text=black]


\begin{document}
\onecolumn
\firstpage{1}

\title[Non-parametric BSSE for NF]{Non-parametric {Bayesian State Space Estimator} for Negative Information} 

\author[\firstAuthorLast ]{\Authors} %This field will be automatically populated
\address{} %This field will be automatically populated
\correspondance{} %This field will be automatically populated

\extraAuth{}% If there are more than 1 corresponding author, comment this line and uncomment the next one.
%\extraAuth{corresponding Author2 \\ Laboratory X2, Institute X2, Department X2, Organization X2, Street X2, City X2 , State XX2 (only USA, Canada and Australia), Zip Code2, X2 Country X2, email2@uni2.edu}


\maketitle


%This leads to additional restrictions on current SLAM methodologies for
%which the parametrisation of the agent’s and object’s joint distribution is a Multivariate Gaussian.

\begin{abstract}

Simultaneous Localisation and Mapping (SLAM) is concerned with the development of filters to accurately 
and efficiently infer the state parameters (position, orientation,...) of an agent and aspects of its
environment, commonly referred to as the map. A mapping system is necessary for the agent to achieve situatedness
which is a precondition for planning and reasoning. In this work we consider an agent who is given the task of
finding a set of objects. The agent has limited perception and can only sense the presences of objects if a
direct contact is made, as a result most of the sensing is negative information. In the absence of recurrent sightings 
or direct measurements of objects there are no correlations from the measurement errors which can be exploited. 
This renders SLAM estimators, for which this fact is their backbone such as EKF-SLAM, ineffective. In addition 
for our setting, no assumptions are taken with respect to the marginals (beliefs) of both the agent and objects (map). 

From the loose assumptions we stipulate regarding the marginals and measurements, we adopt a histogram parametrisation.
We introduce a Bayesian State Space Estimator (BSSE), which we name Measurement Likelihood Memory Filter (MLMF), in
which the values of the joint distribution are not parametrised but instead we directly apply changes from the measurement integration step
to the marginals. This is achieved by keeping track of the history of likelihood functions' parameters. 

We demonstrate that the MLMF gives the same filtered marginals as a histogram filter and show two implementations: MLMF
and scalable-MLMF which both have a linear space complexity. The original MLMF retains an exponential time complexity (although
an order of magnitude smaller than the histogram filter) whilst the scalable-MLMF introduced independence assumption 
such to have a linear time complexity. We further quantitatively demonstrate the scalability of our algorithm with
25 beliefs having up to 10'000'000 states each. In an Active-SLAM setting we evaluate the impact that the size
of the memory's history has on the decision theoretic process in a search scenario for a one step look ahead
information gain planner. We report on both 1D and 2D experiments.

\tiny
 \keyFont{ \section{Keywords:} Negative Information, SLAM, Bayesian State Space Estimator, Histogram-SLAM, Active-exploration} %All article types: you may provide up to 8 keywords; at least 5 are mandatory.
\end{abstract}

\section{Introduction}

% Introduction of SLAM
Estimating the location or state parameters of a mobile agent whilst simultaneously building a map of the environment has been
regarded as one of the most important problems to be solved for agents to achieve true autonomy. It is a necessary precondition for 
any agent to have an estimation of the world at its disposal which accurately encompasses all knowledge and related uncertainties. 
There has been much research surrounding the field of Simultaneous Localisation And Mapping (SLAM) which branches out into a wide variety of sub-fields 
dealing with problems from building accurate noise models of the agent sensors \cite{Plagemann07gaussianbeam}, to determining which environmental 
feature caused a particular measurement, also known as the data association problem \cite{DataAssociation2003} and many more. 

% Why does SLAM Work

Although the amount of research might seem overwhelming at first view, all current SLAM methodologies are founded on a single principle; 
the uncertainty of the map is correlated through the agent's measurements. When an agent localises itself (by reducing position uncertainty)
all previously landmarks have their uncertainty reduced since the uncertainty is correlated with that of the agent's uncertainty.

% The three main pillar of SLAM algorithm and their respective draw backs

There are three main paradigms to solving the SLAM problem. The first is EKF-SLAM (Extendend-Kalman Filter) \cite{SLAM_part1}.
EKF-SLAM models the full state, being the agent's parameters and environmental features, by a Multivariate Gaussian distribution. 
The uncertainty of each individual feature is parametrised by a mean (expected position of the feature) and covariance 
(the level of uncertainty of the position of the feature).

The second approach is Graph-SLAM \cite{TutGraphSLAM}. Graph-SLAM estimates the full path of the agent and considers every measurement to 
be a constraint on the agent's path. It is parameterised by the canonical Multivariate Gaussian. At each time step a new row and column 
is added to the precision matrix which encodes landmarks which have been observed as constraints on the robot's position.
At predetermined times, a nonlinear sparse optimisation is solved to minimise all the accumulated constraints on the robot's path.

The third method is FastSLAM \cite{FastSLAM}. FastSLAM exploits the fact that if we know the agent's position with 
certainty all landmarks become independent. It models the distribution of the agent's position by a particle filter. Each particle
has its own copy of the map and updates all landmarks independently which is the strength of this method. 
However, if many particles are required each must have its own copy of the map. 
It is beyond the scope of this chapter to provide a detailed review of these  three paradigms and the reader is referred to \cite{Thrun_Burgard_Fox_2005}, \cite{SLAM_HBR}.

\subsection{Active-SLAM \& Exploration}

Active-SLAM refers to a decision theoretic process of choosing control actions so as to actively 
increase the convergence of the map. It is used in conjunction with exploration of an unknown environment
in a SLAM setting. The two steps of this process are: (i) generate a set of 
candidate destination positions, (ii) evaluate these positions based on a utility function. The utility  
is a trade off between reducing the uncertainty of the map or reducing the uncertainty
of the agent's position.

Most approaches use a two-level representation of the map in an exploration setting. At the lower level
there is the chosen (landmark-based) SLAM filter and at the higher level a coarser representation of the world.
Such representations can be occupancy grids \cite{Thrun_grid_based_1996} which encode either occupied and free space
or a topological representation \cite{Kollar_2008_Exploration_SLAM}.

Early and current approaches to selecting candidate exploratory locations are based on evaluating 
Next-best-view \cite{Navigation_strategires_for_exploring_indoor_environments} locations. Next-best-view points are 
sampled around \textit{free edges} which are at the horizon of the known map (\textit{frontier} regions). 
In such a setting only target points are generated, not the full trajectory. Probabilistic Road Map (PRM) \cite{PRM_1996}
based methods have been used as planners to reach desired target locations, such as in \cite{RRT-SLAM}, where a Rapidly
Exploring Random Trees (RRT) is combined with FastSLAM. In \cite{ActivePosSLAM}, paths to \textit{frontier} regions are computed
via PRM  on a occupancy grid map and at the lower level they use Pose-SLAM (synonym for Graph-SLAM).

An alternative approach taken to generating candidate locations is the specification of high level macro actions, they being either 
\textit{exploratory} or \textit{revisiting} actions as is the case in \cite{stachniss05robotics}. Macro actions
reduce the costly evaluation of actions, especially in the case of FastSLAM, which requires propagating the filter 
forward in time so as to infer the information gain of each action.

The last approach is to solve the planning problem through formulating it as  Partially Observable Markov Decision Process (POMDP) \cite{Ross08onlineplanning}. 
However all methods take an approximation of the POMDP and consider a one time step planning horizon \cite[p.37]{GeorgiosLidoris}.

There are many ways of generating actions or paths, however their utility is nearly all exclusively based on the \textit{information gain}, 
which is the estimated reduction of entropy a particular action or path would achieve. A few utilities use f-measures such as the Kullback-Leibler divergence. 
Evaluation of different utility metrics are presented in \cite{Active_SLAM_Uncertainty_compar,KL_SLAM_exploration_PF}.

\subsection{Problem Statement}

We consider an agent searching for a set of objects in a partially-known environment, in which exteroceptive feedback is extremely limited.
In the case of our agent, we can think of it as having a range sensor which only provides a response when in direct contact with an object. 
Our agent lives in a \textit{Table Top} world (see Figure \ref{fig:Figure1}) in which is located a set of objects.
The agent's uncertainty of its location and that of the objects is encoded by probability distributions $P(\cdot)$, which 
at initialisation are known as the agent's prior beliefs.


\begin{figure}
  \centering
  \includegraphics[width=0.95\linewidth]{Figure1.jpg}
  \caption{\textbf{Table World} There are three different probability density functions present on the table. 
  The blue represents the believed location of the agent, the red and green probability distributions are associated with object 1 and 2.
  The white shapes in each figure represent the true location of each associated object or agent.}
  \label{fig:Figure1}
\end{figure}
\vspace*{0.6cm}

%	2) Current draw back of all SLAM methodologies 
As the agent explores the world, it integrates all sensing information at each time step and updates its prior beliefs to posteriors
(the result of the prior belief after integrating motion and sensory information).
All current SLAM methods are limited in that they consider only uncertainty induced by sensing inaccuracy inherent in 
the sensor and motion models. In our setting as the sensory information is strictly haptic, we can confidently assume no measurement noise. 
In the search task illustrated in Figure \ref{fig:Figure1}, the beliefs and sparse measurement information available to the agent are 
the source of the uncertainty which is, the absence of positive object measurements. 
This is known as \textbf{negative information} \cite[p.313]{Thrun_Burgard_Fox_2005} \cite{Thrun02particlefilters,negative_info_markov_localisation}. 
Thus SLAM methodologies which use the \textbf{Gaussian error} between the predicted and estimated position of features, such as in the case 
of EKF-SLAM and Graph-SLAM, will not perform well in this setting.  

In addition to the negative sensing information, the original beliefs depicted in Figure \ref{fig:Figure1} are \textbf{non-Gaussian}
and \textbf{multi-modal}. We make \textbf{no assumption} regarding the form of the beliefs. This implies that the joint distribution 
can no longer be parameterised by a Multivariate Gaussian. 
This is an assumption made in many SLAM algorithms, notably EKF-SLAM, and allows for a closed form solution to the state estimation problem. Without the Gaussian assumption 
no closed form solution to the filtering problem is feasible. 
Using standard non-parametric methods (Kernel Density, Gaussian Process, Histogram,...) to represent or estimate the joint distribution becomes
unrealistic after a few dimensions or additional map features. 
FastSLAM could be a potential candidate, however as it parameterises the position uncertainty of the agent by a particle filter and each
particle has its own copy of the map, the memory demands become quickly significant.  For planning purposes we would also want to have a 
single representation of the marginals. The box below summarises the desirable attributes and assumptions for our filter.

\begin{center}
\begin{tikzpicture}    
\node [white_box] (box){%
\begin{minipage}{0.95\textwidth}
\begin{itemize}
  \item Non-Gaussian joint distribution, no assumptions are made with respect to its form.
  \item Mostly negative information available (absence of positive sightings of the landmarks).
  \item Joint distribution volume grows exponentially with respect to the number of objects and states.
  \item Joint distribution volume is dense, there is high uncertainty.
\end{itemize}
\end{minipage}

};
\node[fancytitle, right=10pt] at (box.north west) {Attributes \& Assumptions};
\end{tikzpicture}%
\end{center}

\subsection{The main contribution to the field of Artificial Intelligence} 

In a wide range of Artificial Intelligence (AI) applications the agent's beliefs are discrete. This non-parametric representation
is the most unconstraining but comes at a cost. The parameterisation of the belief's joint distribution grows at the rate of a double exponential.
We propose a Bayesian State  Space Estimator (BSSE) which delivers the same filtered beliefs as a traditional filter but without explicitly parametrising the 
joint distribution. We refer to our novel filter as the Measurement Likelihood Memory Filter (MLMF). 
It keeps track of the history of measurement likelihood functions, referred to as the memory, which 
have been applied on the joint distribution.
The MLMF filter efficiently processes negative information. To the author's knowledge there has been little
research on the integration of negative information in a SLAM setting. Previous work considered the case of active localisation \cite{NegInfoFurtherStudies}.
The incorporation of negative information is useful in many contexts and in particular in Bayesian Theory of Mind, \cite{Bake_Saxe_Tene_2011},
where the reasoning process of a human is inferred from a Bayesian Network and in our own work \cite{deChambrier2013} where we model the 
search behaviour of a intentionally blinded human. In such a setting much negative information is present and an efficient belief filter is required. 
Our MLMF is thus applicable to the SLAM \& AI community in general and to the Cognitive Science community which models human or agent behaviours through 
the usage of Bayesian state estimators.

By using this new representation we implement a set of passive search trajectories through the state 
space and demonstrate, for a discretised state space, that our novel filter is optimal with respect to the Bayesian criteria (the successive
filtered posteriors are exact and not an approximation with respect to Bayes rule). We provide an analysis of the space and time complexity of 
our algorithm and prove that it is always more efficient even when considering worst case scenarios.
Lastly we consider an Active-SLAM setting and evaluate how constraining the size of the number of memorised likelihood 
functions impacts the decision making process of a greedy one-step look-ahead planner.

The rest of the document is structured as follows: in section 2, we overview the Bayes filter recursion and apply it to a simple 
1D search scenario for both a discrete and Gaussian parametrisation of the beliefs. We demonstrate that discrete parametrisation gives
the correct filtered beliefs but at a very high cost and that the EKF-SLAM fails to provide the adequate solution. Section 3 we 
introduce the Measurement Likelihood Memory Filter and overview its parametrisation. Section 4 we derive the computational time and 
space complexity of the MLMF. Section 5 describes additional assumptions made with respect to the MLMF to make it 
scalable (scalable-MLMF). In section 6 we numerically evaluate the time complexity of the scalable-MLMF and check the assumption we made 
for it to be scalable. We investigate the filter's sensitivity with respect to its parameters in an Active-SLAM setting.

\section{Bayesian State Space Estimation}\label{sec:BSSE}

Bayesian State Space Estimation (BSSE) focuses on incorporating observations to update a prior distribution to a posterior distribution 
over the state space through the application of Bayes probability rules. The agent's random variable, $A$, 
is associated with the uncertainty of its location in the world, the same holds for the object(s') random variable(s), $O$. 
Given a sequence of actions and observations, $\{u_{1:t},y_{0:t}\}$ (subscript $0:t$ is all the indexed variables from $t=0$ to the current time $t=t$), 
algorithms of the BSSE family incorporate this information to provide an estimate $P(A_t,O|Y_{0:t},u_{1:t})$. This is known as the 
filtering problem where all past information is incorporated to estimate the current state.  

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{Figure2}
\caption{Directed graphical model of dependencies between the agent(A) and object(O)'s estimated location. Each 
object, $O^{(i)}$ is associated with one sensing random variable $Y^{(i)}$. The overall sensing random variable is $Y = \left[Y^{(1)},\dots,Y^{(M-1)}\right]^{\mathrm{T}}$,
where $M$ is the total number of agent and object random variables in the filter. 
For readability we have left out the time index $t$ from $A$ and $Y$. Since the objects are static, they have no temporal process associated with 
them thus they will never have a time subscript. The two models necessary for filtering are the motion model $P(A_t|A_{t-1},u_t)$ (red) and measurement model
$P(Y_t|A_t,O)$ (blue).}
\label{fig:bayesian_sse_dag}
\end{figure}

In Figure \ref{fig:bayesian_sse_dag} we depict the general Bayesian Network (BN) of a BSSE. The BN conveys two types of
information, the dependence and independence relation between the random variables in the graph which can be established
through \textit{d-separation} \cite{BayesBall}. The \textbf{conditional dependence} $A \dependent O | Y$ is key to all BSSE and SLAM algorithms. 
The strength of the dependence between the agent and object random variable is governed by the measurement likelihood $P(Y_t|A_t,O)$. 
If the measurement likelihood does not change the joint distribution, then the agent and object random variables will be independent, $A \independent O$. 
If they are independent, then no information acquired by the agent can be used to infer changes in the object estimates.
    
We next demonstrate the behaviour of the BN joint distribution, Figure \ref{fig:bayesian_sse_dag}, for two different parameterisations 
in the case of the absence of direct sighting of the object by the agent.

\subsection{EKF-SLAM}\label{sec:EKF-SLAM}

In EKF-SLAM the joint density $p(A_{t},O|Y_{0:t},u_{1:t}) = g([A_t,O]^{\mathrm{T}};\mu_t,\Sigma_t)$ is parametrised by a single Gaussian 
function $g$ with mean, $\mu_t = \left[\mu_{A_{t}},\mu_{O^{(1)}},\dots,\mu_{O^{(M-1)}}\right]^{\mathrm{T}} \in \mathbb{R}^{3 + 2\cdot (M-1)}$  where the 
object random variables are in $\mathbb{R}^2$, and covariance, $\Sigma_t$. The mean value of the agent 
$\mu_a = [x,y,\phi]^{\mathrm{T}} \in \mathbb{R}^3$ and those of the objects are $\mu_{O^{(i)}} = [x,y]^{\mathrm{T}} \in \mathbb{R}^2$.

\begin{equation}
\Sigma_t = \begin{bmatrix}
       \Sigma_a & \Sigma_{ao}  \\[0.3em]
       \Sigma_{oa} & \Sigma_o
     \end{bmatrix}
     \in \mathbb{R}^{(3 + 2\cdot (M-1)) \times (3 + 2\cdot (M-1))}
\end{equation}

The $j$'th object measurement is described by range and bearing  $Y^{(j)}_t = [r,\phi]$ in the frame of reference of the agent.
EKF-SLAM assumes that the measurement is corrupted by Gaussian noise, $\epsilon \sim \mathcal{N}(0,R)$,
resulting in the likelihood function:
\begin{align} 
   p(Y_t|A_t,O_t) &= \frac{1}{|2\pi R|^{\frac{1}{2}}} \exp \left( -\frac{1}{2} \big(Y_t - \hat{Y}_t\big)^{\mathrm{T}}R^{-1}\big(Y_t - \hat{Y}_t\big) \right)\label{eq:lik-measurement}\\
   \hat{Y}_t      &= \exp\left(-\frac{1}{2\sigma^2} ||A_t - O ||^2 \right)\label{eq:measurement_ekf}
\end{align}
where the covariance, $R$, encompasses the uncertainty in the measurement and Equation \ref{eq:measurement_ekf} is the measurement function. The elements of the covariance matrix capture 
the measurement error between the true $Y$ and expected $\hat{Y}$ range and bearing of the object. As the joint distribution 
is parametrised by a single Multivariate Gaussian, a closed form solution to the filtering Equations exists, called the Kalman 
Filter \cite{SLAM_part1}. 

The error between the true and expected measurement $e = (Y_t - \hat{Y}_t)$ is an important part of the application of EKF-SLAM.
In our scenario the agent can only perceive the objects once he enters in direct contact with them. 
This means that the variance of the observation $Y_t$ will always be equal to $\hat{Y}$ until a contact occurs. 
To illustrate the problems which this gives rise to, we give an illustration of a 1D search. Figure \ref{fig:EKF-SLAM} shows the 
resulting updates of the beliefs for 4 chosen time segments.

\begin{figure}
\centering
 \includegraphics[width=0.9\textwidth]{Figure3}
\caption{\textbf{a)} EKF-SLAM time slice evolutions of the pdfs. 
The temporal ordering is given by the numbers in the top right corner of each plot.
The blue pdf represents the agent's believed location and the circle is the agent's true location. The same holds 
for the red distribution which represents the agent's belief of the location of an object.
\textbf{b)} Evolution of the covariance components of $\Sigma$ over time and true $Y_t$ and expected measurements,  $\hat{Y}_t$. 
$\Sigma_a$ and $\Sigma_o$ are the variances of the agent and object positions and $\Sigma_{ao}$ is the cross-covariance 
term.}
\label{fig:EKF-SLAM}
\end{figure}

As expected we do not get the desired behaviour, that the beliefs start updating as soon as they are overlapping, 
see 2nd-3rd temporal snapshot in the Figure. 
Even when most of the belief mass of the agent's location pdf overlaps that of the object pdf, no belief update occurs. 
The multivariate Gaussian parameterisation only guarantees a dependency between the agent and object random variables 
when there is a positive sighting of the landmarks.  This can been seen in Figure \ref{fig:EKF-SLAM} (b),
where the component $\Sigma_{ao}$ is 0 most of the time which implies that $A \independent O | Y$ which is undesirable. 

\subsection{Histogram-SLAM}\label{sec:Discrete}

In Histogram-SLAM, the joint distribution is discretized and each bin has a parameter, 
${P(A_t=i,O=j|Y_{0:t},u_{1:t};\boldsymbol{\theta}) = \boldsymbol{\theta}^{(ij)}}$, which sums to one, $\sum_{ij} \boldsymbol{\theta}^{(ij)} = 1$. 
For shorthand notation we will write $P(A_t,O|Y_{0:t},u_{1:t})$ instead of $P(A_t=i,O=j|Y_{0:t},u_{1:t};\boldsymbol{\theta})$.
The probability distribution of the agent's position is given by marginalising the object random variable:
\begin{equation}
 P(A_t|Y_{0:t},u_{1:t};\ThA)    = \sum\limits_{j=1}^{|O|} P(A_t,O=j|Y_{0:t},u_{1:t};\boldsymbol{\theta}) \label{eq:agent_marginal}
\end{equation}

% For ease of notation we use the shorthand $P(A_t)$ for $P(A_t|Y_{0:t},u_{1:t};\ThA)$. 
The converse holds true for the object's marginal, that is the summation would be over 
the agents variable. Figure \ref{fig:histogram_joint} (\textit{Top}) illustrates the joint distribution of both the agent and the object random variable. 
The 1D world of the agent and object is discretised to 10 states, producing a joint distribution with 100 parameters!
For a state space of $N$ bins, $i=1...N$, and there is a total of $M$ random variables (one agent and $M-1$ objects)
and the joint distribution has $N^{M}$ parameters. This exponential increase renders Histogram-SLAM intractable
with this parameterisation.

\begin{figure}
 \centering
 \includegraphics[width=0.8\textwidth]{Figure4}
 \caption{\textbf{Top}: \textit{Left:} Initialisation of the agent and object joint distribution. 
 \textit{Right:} Marginals of the agent and object parameterised by $\ThA$ and $\ThO$, giving the probability of their location. The marginal of each 
 random variable is obtained from Equation \ref{eq:agent_marginal}. The probability of
 the agent and object being in state $s=6$ is given by summing the blue and red highlighted parameters in the joint distribution. 
 \textbf{Bottom}: 1D world Likelihood $P(Y_t|A_t,O)$, the white regions $A \cap O$ will leave the joint distribution unchanged whilst
 the black regions will evaluate the joint distribution to zero. \textit{Left:} No contact detected with the object, the current measurement 
 is $Y_t = \textcolor{red}0$, both the agent and object cannot be in the same state. \textit{Right:} The agent 
 entered into contact with the object and received a haptic feedback $Y_t = \textcolor{red}1$. The agent receives 
 only two measurement possibilities, contact or no contact.
 }
 \label{fig:histogram_joint}
\end{figure}

In the tasks we consider, an observation occurs only if the agent enters in contact with the object, which implies that both
occupy the same discrete state. The likelihood function $P(Y_t|A_t,O)$ is:

\begin{equation} \label{eq:discrete_likelihoood}
P(Y_t=1|A_t,O) =
  \begin{cases}
    1       & \quad \text{if } A_t = O     \\
    0  	    & \quad \text{if } A_t \not= O \\
  \end{cases}
\end{equation}

Figure \ref{fig:histogram_joint} (\textit{Bottom left}), illustrates the likelihood function, Equation \ref{eq:discrete_likelihoood}, 
in the case when a no contact measurement $Y_t=0$. When there is no measurement all the parameters of the 
joint distribution which are in the black regions become zero, which we refer to as the \textbf{dependent states} $A \cap O$ of the joint 
distribution. The white states are the \textbf{independent states} $A \ominus O$, they are not changed by the likelihood function 
and the values of the joint distribution in those states, $P_{\cap}(A_t,O|Y_{0:t},u_{1:t})$, will be unchanged by the likelihood function
$P_{\ominus}(A_t,O|\mathbf{Y_{0:t}},u_{1:t}) \propto P_{\ominus}(A_t,O|\mathbf{Y_{0:t-1}},u_{1:t})$. 
When the object is detected (\textit{Bottom right}) the likelihood constrains all non-zero values of the joint 
distribution to be in states $i = j$, which in the case of a 2-dimensional joint 
distribution is a line. The \textbf{sparsity} of the likelihood function  will be key to the development of the MLMF filter.
Two models are needed to perform the recursion, namely the motion model $P(A_t|A_{t-1},u_t)$ and the measurement model
$P(Y_t|A_t,O)$, which we already detailed. Both models applied consecutively to the initial joint distribution results in a posterior
distribution. Both Equation \ref{eq:disc_motion}-\ref{eq:disc_measurement} are part of the Histogram Bayesian filter 
update:
\begin{center}
\begin{tikzpicture}    
\node [white_box] (box){%
\begin{minipage}{0.8\textwidth}
\vspace*{-1cm}
\begin{align}
 &\mathrm{\textbf{intialisation}}\nonumber\\
 &P(A_0,O;\boldsymbol{\theta}) = P(A_0;\ThA)\, P(O;\ThO) = \ThA \times \ThO \label{eq:ch5:disc_prod_AO}\\
 &\mathrm{\textbf{motion}}\nonumber\\
 &P(A_t,O|Y_{0:t-1},u_{1:t}) = \sum_{A_{t-1}} P(A_t|A_{t-1},u_t)\, P(A_{t-1},O_t|Y_{0:t-1},u_{1:t-1} \label{eq:disc_motion})\\
 &\mathrm{\textbf{measurement}}\nonumber\\
 & P(A_t,O|Y_{0:t},u_{1:t}) = \frac{P(Y_t|A_t,O)\, P(A_t,O|Y_{0:t-1},u_{1:t}) }{P(Y_t|Y_{0:t-1},u_{1:t})} \label{eq:disc_measurement} 
\end{align}
\end{minipage}
};
\node[fancytitle, right=10pt] at (box.north west) {Histogram Bayesian recursion};
\end{tikzpicture}%
\end{center}

Figure \ref{fig:discrete_example} illustrates the evolution of the joint distribution in a 1D example. 
The agent and object's true positions (unobservable) are in state 6 and 1. The agent moves three steps towards state 10. At each time 
step, as the agent fails to sense the object, the likelihood function  $P(Y_t=0|A_t,O)$ (Figure \ref{fig:histogram_joint}, \textit{Bottom left})
is applied. As the agent moves towards the right, the motion model shifts the joint distribution towards state 10 along the agent's 
dimension, $i$ (note that state 1 and 10 are wrapped).

\begin{figure}
 \centering
  \includegraphics[width=0.8\textwidth]{Figure5}
  \caption{Histogram-SLAM, 4 time steps. \textbf{1} Application of likelihood $P(Y_0=0|A_0,O)$ and the agent remains stationary, all states along the green line become zero.
  \textbf{2} The agent moves to the right $u_1=1$, the motion $P(A_1|A_0,u_1)$, and likelihood models are applied consecutively. The right motion results in a shift (black arrow on the left) in the joint probability 
  distribution towards the state $i=10$. All parameters on the pink line are zero. \textbf{3} Same as two. At each time step a new likelihood function (pink line) is applied to the joint distribution.}
  \label{fig:discrete_example}
\end{figure}

As the agent moves to the right more joint distribution parameters become zero. The re-normalisation by the \textbf{evidence} ($P(Y_t|Y_{0:t-1},u_{1:t})$, denominator of Equation \ref{eq:disc_measurement}), 
which increases the value of the remaining parameters, is equal to the sum of the probability mass which was set to zero by the likelihood function.
Thus the values of the parameters of the joint distribution which fall on the pink line in Figure \ref{fig:discrete_example} 
(green line also, but only for first time slice) become zero and their values are redistributed to the remaining non-zero parameters. 

The \textbf{inconvenience} with Histogram-SLAM is that its time and space complexity is exponential as the joint distribution is discretised and 
parametrised by $\boldsymbol{\theta}^{(ij)}$. Instead we propose a new filter, MLMF, which we formally introduce in the next section. This filter
achieves the same result as the Histogram filter but without having to parameterise the values of the joint distribution, thus avoiding the exponential growth cost. 

The \textbf{key idea} behind the mechanism of the MLMF filter is to evaluate only the joint distribution $P_{\cap}(A_t,O|Y_{0:t},u_{1:t})$ in
dependent states and updates directly the marginals without marginalising the entire joint state space.
The MLMF filter parametrises \textbf{explicitly} the marginals ${P(A_t|Y_{0:t},u_{1:t};\ThA)}$, ${P(O|Y_{0:t},u_{1:t};\ThO)}$. 
This contrasts the Histogram filter where the marginals are derived from the joint distribution by marginalisation over the entire joint state space. 

\section{Measurement Likelihood Memory Filter}\label{sec:MLMF}
 
MLMF keeps a  \textbf{function parameterisation} of the joint distribution instead of a \textbf{value parameterisation} as it is the case 
for Histogram-SLAM. At initialisation the joint distribution is represented by the product of marginals, Equation \ref{eq:prod_AO}, which 
would result in the joint distribution illustrated in Figure \ref{fig:histogram_joint}, if it were to be evaluated at all states $(i,j)$
as it is done for Histogram-SLAM. MLMF will only evaluate this product, when necessary, at specific states. 
At each time step the motion and measurement update are applied, Equation \ref{eq:mlmf_motion_update}-\ref{eq:mlmf_measurement_update}.
An important distinction is that these updates are performed on the \textbf{un-normalised} joint distribution ${P(A_t,O,Y_{0:t}|u_{1:t})}$, which is not the case in Histogram-SLAM where 
the updates are done on the conditional ${P(A_t,O|Y_{0:t},u_{1:t}})$. After applying multiple 
motion and measurement updates the resulting joint distribution is given by Equation \ref{eq:mlmf_filter_conditional}, see Appendix \ref{appendix:recursion_example}
for a step-by-step derivation. 
 
\begin{center}
\begin{tikzpicture}    
\node [white_box] (box){%
\begin{minipage}{0.85\textwidth}
\vspace*{-1cm}
\begin{align}
 &\mathrm{\textbf{joint marginals (initial)}}\nonumber\\
 &P(A_0,O) = P(A_0;\mathcolor{blue}{\ThAs})\, P(O;\mathcolor{red}{\ThOs}) \label{eq:prod_AO}\\
 &\mathrm{\textbf{motion}}\nonumber\\
 &P(A_t,O,Y_{0:t-1}|u_{1:t}) = \sum_{A_{t-1}} P(A_t|A_{t-1},u_t)\, P(A_{t-1},O,Y_{0:t-1}|u_{1:t-1})  \label{eq:mlmf_motion_update} \\
 &\mathrm{\textbf{measurement}}\nonumber\\
 &P(A_t,O,Y_{0:t}|u_{1:t};\mathcolor{red}{\ThOs},\mathcolor{blue}{\ThAs},\mathcolor{dark-green}{\boldsymbol{\Psi_{0:t}}}) = \nonumber \\[0.2cm]
 &\hspace*{1cm} P(Y_t|A_t,O)\,P(O;\mathcolor{red}{\ThOs})\, P(A_t|u_{1:t};\mathcolor{blue}{\ThAs})\, P(Y_{0:t}|A_t,O,u_{1:t};\mathcolor{dark-green}{\boldsymbol{\bar{\Psi}_{0:t}}})  \label{eq:mlmf_measurement_update} \\
 &\mathrm{\textbf{joint}}\nonumber\\
 &P(A_t,O|Y_{0:t},u_{1:t};\mathcolor{red}{\ThOs},\mathcolor{blue}{\ThAs},\mathcolor{dark-green}{\boldsymbol{\Psi_{0:t}}},\mathcolor{dark-blue}{\BAlph}) = \frac{P(A_t,O,Y_{0:t}|u_{1:t};\mathcolor{red}{\ThOs},\mathcolor{blue}{\ThAs},\mathcolor{dark-green}{\boldsymbol{\Psi_{0:t}}})}{P(Y_{0:t}|u_{1:t};\mathcolor{dark-blue}{\BAlph}) } \label{eq:mlmf_filter_conditional}  \\ 
 &\mathrm{\textbf{filtered marginal}}\nonumber\\
 &P(A_t|Y_{0:\mathbf{t}};\mathcolor{blue}{\ThA}) = P(A_t|Y_{0:\mathbf{t-1}};\mathcolor{blue}{\ThA}) - \Big(P_{\cap}(A_t|Y_{0:t-1}) -  P_{\cap}(A_t|Y_{0:t})  \Big)   \label{eq:marignal_mrf}  \\
 &P(O|Y_{0:\mathbf{t}};\mathcolor{red}{\ThO}) = P(O|Y_{0:\mathbf{t-1}};\mathcolor{red}{\ThO}) - \Big(P_{\cap}(A_t|Y_{0:t-1}) -  P_{\cap}(A_t|Y_{0:t})  \Big)    
 \end{align}
\end{minipage}
};
\node[fancytitle, right=10pt] at (box.north west) {MLMF Bayesian filter};
\end{tikzpicture}%
\end{center}

The MLFM filter is parameterised by the agent and object 
\textbf{joint marginals} $P(A_t|u_{1_:t};\mathcolor{blue}{\ThAs})$, $P(O;\mathcolor{red}{\ThOs})$, the \textbf{filtered marginals}  
$P(A_t|Y_{0:t},u_{1:t};\mathcolor{blue}{\ThA})$ ($u_{1:t}$ not shown in the above box), $P(O|Y_{0:\mathbf{t}};\mathcolor{red}{\ThO})$, 
the evidence $P(Y_{0:t}|u_{1:t};\mathcolor{dark-blue}{\BAlph})$ and the history of likelihood functions, $P(Y_{0:t}|A_t,O,u_{1:t};\mathcolor{dark-green}{\BPsi})$ Equation \ref{eq:memory}, which is 
the product of all the likelihood functions since $t=0$ until $t$ and we will refer to it as the \textbf{memory likelihood function}: 
\begin{equation}
 P(Y_{0:t}|A_t,O,u_{1:t};\mathcolor{dark-green}{\BPsi}) := \prod_{i=0}^t P(\mathcolor{dark-green}{Y_i}|A_t,O,u_{i+1:t};\mathcolor{dark-green}{l_i}) \label{eq:memory}
\end{equation}

\begin{equation} \label{eq:ch5:liklihood_v2}
P(\mathcolor{dark-green}{Y_i}=0|A_t,O,u_{i+1:t};\mathcolor{dark-green}{l_i}) :=
  \begin{cases}
    0       & \quad \text{if } A_t + \mathcolor{dark-green}{l_i} = O     \\
    1  	    & \quad \text{else}  \\
  \end{cases}
\end{equation}
\begin{equation}
  \mathcolor{dark-green}{l_i} := \sum\limits_{j=i+1}^t u_j  \label{eq:offset}
\end{equation}

The memory likelihood function's parameters $\BPsi = \{(Y_i,l_i)\}_{i=0:t}$ consist of a set of measurements $Y_{0:t}$ and offsets $l_{0:t}$
depicted in greed. The measurements $Y_i \in \{0,1\}$ are always binary, whilst the offsets $l_i$, actions $u_t$, 
agent $A_t$ and object $O$ variables' size are equal to the dimension of the state space. The subscript $i$ 
of an offset $l_i$ indicates which likelihood function it belongs to. The offset of a likelihood function is given by the 
summation of all the applied actions from the time the likelihood was added until the current time $t$, Equation \ref{eq:offset}, which can be computed recursively.
The motion update, Equation \ref{eq:mlmf_motion_update}, when applied to the joint distribution results in the 
initial marginal $P(A_0;\ThAs)$ and the likelihood functions being moved along the agent's axis. In Algorithm \ref{alg:memory-motion}, we detail how an action $u_t$ and measurement $Y_t$, result in the update of
the memory likelihood's parameters from $\Psi_{0:t-t}$ to $\Psi_{0:t}$; this is an implementation of 
Equations \ref{eq:mlmf_motion_update}-\ref{eq:mlmf_measurement_update}.

\begin{center}
\begin{minipage}{.65\linewidth}

\begin{algorithm}[H]
\label{alg:memory-motion}
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}

\Input{$\Psi_{0:t-1}$, $Y_t$, $u_t$}
\Output{$\Psi_{0:t}$}
\BlankLine
\nonl\hrulefill\\
\nonl\textbf{motion update} $\bar{\Psi}_{0:t} \gets \Psi_{0:t-1}$ \label{alg:motion_memory}\\
\For{$l_i \in \Psi_{0:t-1}$}{$l_i = l_i + u_t$}
\nonl\hrulefill\\
\nonl\textbf{measurement update}\\
$\Psi_{0:t} \gets \{\bar{\Psi}_{0:t}, (Y_t,l_t:=0)\}$ 
\caption{Memory Likelihood update}

\end{algorithm} 
\end{minipage}
\end{center}

Figure \ref{fig:maringal_joint_example_v2} illustrates the evolution of the \textbf{un-normalised} MLMF joint distribution ${P(A_t,O,Y_{0:t}|u_{1:t})}$, Equation  \ref{eq:mlmf_filter_conditional}.
For ease of notation we will omit at times the parameters of the probability functions.
Both $P(A_0;\mathcolor{blue}{\ThAs})$ and $P(O;\mathcolor{red}{\ThOs})$ were initialised as for the Histogram-SLAM example in Figure \ref{fig:discrete_example} on page \pageref{fig:discrete_example}.
Two actions $u_{1:2}=1$ are applied and three measurements $Y_{0:2} = 0$ received which are then integrated into the filter. 
Since initialisation of the joint distribution at $t=0$ until $t=2$ the object's marginal $P(O;\mathcolor{red}{\ThOs})$ remains unchanged and the agent's 
marginal $P(A_2|u_{1:2};\mathcolor{blue}{\ThAs})$ is updated by the two actions according to the motion update, see 
Figure \ref{fig:maringal_joint_example_v2} (\textit{Top-right}).
The product of these two marginals (terms of Equation \ref{eq:mlmf_filter_conditional} before the memory likelihood product) results in the joint
probability distribution $P(A_2,O|u_{1:2};\mathcolor{blue}{\ThAs},\mathcolor{red}{\ThOs})$ illustrated in 
Figure \ref{fig:maringal_joint_example_v2} (\textit{Middle-right}). 

\begin{figure}
 \centering
 \includegraphics[width=0.65\textwidth]{Figure6}
 \caption{Un-normalised MLMF joint distribution, numerator of Equation \ref{eq:mlmf_filter_conditional}, at time $t=3$.
 Three measurements (all $Y=0$) and two actions (both $u=1$) have been integrated into the joint distribution, for simplicity we do not consider any motion noise. \textit{Left column:} The first plot
 illustrates the likelihood of the first measurement $Y_0$. We highlight the contour in light-green to indicate that it was the first applied likelihood 
 function (see the correspondence with Figure \ref{fig:discrete_example}). The first likelihood function has been moved by the 2 actions, the 
 second likelihood function has been moved by one action (the last one, $u_2=1$) and the third likelihood has had no action applied to it 
 yet. The last applied likelihood function is highlighted in pink and the product of all the likelihoods since $t=0$ until $t=3$ is depicted at the 
 bottom of the figure which is $P(Y_{0:2}|A_2,O,u_{1:2})$. \textit{Right column:} the top figure illustrates the original marginal of the 
 object $P(O;\ThOs)$, which remains unchanged, and the agent's marginal $P(A_2|u_{1:2};\ThAs)$ which has moved in accordance to the motion update function. 
 Their product would results in the joint distribution $P(A_2,O|u_{1:2};\ThAs,\ThOs)$ illustrated in the middle figure if evaluated at each state $(i,j)$. The bottom figure is the result
 of multiplying $P(A_2,O|u_{1:2};\ThAs,\ThOs)$ with  $P(Y_{0:2}|A_2,O,u_{1:2};\boldsymbol{\Psi_{0:2}})$ giving the filtered joint distribution, Equation \ref{eq:mlmf_filter_conditional}.
 }
 \label{fig:maringal_joint_example_v2}
\end{figure}
 
In the left column of Figure \ref{fig:maringal_joint_example_v2} we illustrate how the memory likelihood term, Equation \ref{eq:memory}, 
is updated according to Algorithm \ref{alg:memory-motion}. In the \textit{Top-left}, the first likelihood function 
$P(\mathcolor{dark-green}{Y_0}|A_2,O,u_{1:2};\mathcolor{dark-green}{l_0})$ is illustrated. As two actions have been applied, Algorithm \ref{alg:memory-motion} is applied 
twice which results in a $\mathcolor{dark-green}{l_0}=2$ parameter for the first likelihood function. In the figure we highlighted the likelihood in light-green 
to indicate that it was the first added to the memory term making it convenient to compare to Figure \ref{fig:discrete_example} on page \pageref{fig:discrete_example}. As for the second 
likelihood function $P(\mathcolor{dark-green}{Y_1}|A_2,O,u_{2};\mathcolor{dark-green}{l_1})$ only one action has been applied and the third likelihood 
function $P(\mathcolor{dark-green}{Y_2}|A_2,O;\mathcolor{dark-green}{l_2=0})$ has not yet been updated by the next action. 
The parameters of the memory likelihood function, Equation \ref{eq:memory}, are: $\Psi_{0:2} = \{(0,2)_{i=0},(0,1)_{i=1},(0,0)_{i=2}\}$ and its evaluation 
is illustrated in the \textit{Bottom-left} of Figure \ref{fig:maringal_joint_example_v2}. 
  
The reader may have noticed that the amplitude of the values of the filtered joint distribution illustrated in Figure \ref{fig:maringal_joint_example_v2} have changed
when compared with Figure \ref{fig:discrete_example}, but not the structure. This is because we have not re-normalised the joint distribution by the evidence $P(Y_{0:t}|u_{1:t};\alpha_{0:t})$. We will show 
in the next section how we can \textbf{recursively} compute the evidence without having to integrate the whole joint distribution which would be 
expensive.

Our goal is to be able to compute the marginals $P(A_t|Y_{0:t},u_{1:t};\ThA)$, $P(O|Y_{0:t};\ThO)$ of the agent and object random variables and 
evidence $P(Y_{0:t}|u_{1:t};\alpha_{0:t})$ \textbf{without} having to perform an \textbf{expensive marginalisation} over the entire space of the joint distribution 
as was the case for Histogram-SLAM.  The next section describes how to efficiently compute the evidence and the marginals.
For ease of notation we will not always show the conditioned actions $u_{1:t}$, so $P(A_t,O|Y_{0:t},u_{1:t})$ will 
be $P(A_t,O|Y_{0:t})$.

\subsection{Evidence and marginals}

In order to compute efficiently the marginal likelihood (also known as evidence) $P(Y_{0:t}|u_{1:t};\alpha_{0:t})$ and the filtered  marginals $P(A_t|Y_{0:t},u_{1:t};\ThA)$,
$P(O|Y_{0:t};\ThO)$ we take advantage of the fact that only a very small area 
in the joint distribution space will be affected by the measurement likelihood function at each time step.
Without lost of generality the likelihood function will only make a difference to dependent $A \cap O$ states in the joint distribution, states 
where the likelihood function is less than one. The states inside $A \ominus O$ will not be affected, where the likelihood function 
is equal to one.

\begin{equation}\label{eq:joint_independent_dependent}
 P(A_t,O|Y_{0:t}) = P_{\cap}(A_t,O|Y_{0:t}) + P_{\ominus}(A_t,O|Y_{0:t})
\end{equation}

This formulation will lead to large computational gain as the independent term is not influenced by the measurement function: 
${P_{\ominus}(A_t,O,\mathbf{Y_{0:t}}) = P_{\ominus}(A_t,O,\mathbf{Y_{0:t-1}})}$ and 
${P_{\ominus}(A_t,O|\mathbf{Y_{0:t}}) \propto P_{\ominus}(A_t,O|\mathbf{Y_{0:t-1}})}$.

\subsubsection{Evidence}

The evidence of the measurement $P(Y_{0:t}|u_{1:t};\alpha_{0:t})$ is the normalisation coefficient of the joint distribution Equation \ref{eq:mlmf_filter_conditional}.
It is the amount of probability mass re-normalised to the other parameters as a result of the consecutive application of the likelihood function.
At time step $t$, the normalising factor to be added to the evidence is the difference between the probability mass located 
inside $A\cap O$ before and after the application of the measurement function $P(Y_t|A_t,O)$, 
see Equation \ref{eq:I_v1}-\ref{eq:I_v2} (see Appendix \ref{appendix:evidence} for the full derivation).

\begin{align}
 \alpha_t 			 	&= \sum\limits_{A_t}\sum\limits_{O} \Big(P(Y_t|A_t,O) - 1\Big)P_{\cap}(A_t,O,Y_{0:t-1}|u_{1:t}) \label{eq:I_v1} \\
 P(Y_{0:t}|u_{1:t};\mathcolor{dark-blue}{\BAlph})        &= 1 + \underbrace{\alpha_{0:t-1} + \alpha_t}_{\alpha_{0:t}} \label{eq:I_v2}
\end{align}

The advantage of Equation \ref{eq:I_v1} is that the summation is only over the states which are in the dependent area $\cap$ of the joint 
distribution. Until an object is sensed, the likelihood will always be zero $P(Y_t|A_t,O) = 0$ and $\alpha_t$ will correspond to the probability 
mass which falls within the region of the joint distribution in which the likelihood function is zero. 
The point of interest is that as we perform the filtering process we will never re-normalise the whole joint distribution, but only keep 
track of how much it should have been normalised. To this end the marginals $P(A_t|u_{1:t};\ThAs)$ and $P(O;\ThOs)$  are never re-normalised but are used
at each step to compute how much of the probability mass $\alpha_t$ should go to the normalisation factor $P(Y_{0:t}|u_{1:t};\mathcolor{dark-blue}{\BAlph})$. 
The normalisation factor in question will never be negative, as the joint distribution sums to one and each $\alpha_t$ represents some of the mass removed from the joint distribution. Since we 
keep track of the history of applied  measurement likelihood functions the same amount of probability mass is never removed twice
from the joint distribution.

\subsubsection{Marginals}

There are two different sets of marginals used in the MLMF filter. The first set are the \textbf{joint marginals} of the joint distribution, Equation \ref{eq:mlmf_filter_conditional}
parameterised by $\mathcolor{blue}{\ThAs}$ and $\mathcolor{red}{\ThOs}$. The second set of marginals are the \textbf{filtered marginals} which are updated by evaluating the joint distribution in dependent states and 
are parameterised by $\mathcolor{blue}{\ThA}$ and $\mathcolor{red}{\ThO}$. At initialisation before the the first action or observation is made 
the parameters of the filtered marginal are set equal to those of the joint distribution. MLMF takes advantage of the sparsity of the likelihood function which results in only the dependent elements of the marginal being affected, 
Equation \ref{eq:marignal_mrf} (see Appendix \ref{appendix:marginal} for the full derivation of Equation \ref{eq:marignal_mrf}). 

\begin{equation}
 P(O|Y_{0:\mathbf{t}};\mathcolor{red}{\ThO}) = P(O|Y_{0:\mathbf{t-1}};\mathcolor{red}{\ThO}) - \Big(P_{\cap}(O|Y_{0:t-1}) -  \mathbf{P_{\cap}(O|Y_{0:t})}  \Big)   \label{eq:marignal_mrf}  
\end{equation}
\begin{align}
 &\mathbf{P_{\cap}(O|Y_{0:t}};\mathcolor{blue}{\ThAs},\mathcolor{red}{\ThOs},\mathcolor{dark-green}{\BPsi},\mathcolor{dark-blue}{\BAlph})  = \sum\limits_{A_t} \mathbf{P_{\cap}(A_t,O|Y_{0:t},u_{1:t}};\mathcolor{blue}{\ThAs},\mathcolor{red}{\ThOs},\mathcolor{dark-green}{\BPsi},\mathcolor{dark-blue}{\BAlph}) \nonumber \\[0.5cm]
 &\hspace*{2cm} =\frac{ \sum\limits_{A_t} P_{\cap}(O;\mathcolor{red}{\ThOs}) P_{\cap}(A_t|u_{1:t};\mathcolor{blue}{\ThAs})  P(Y_{0:t}|A_t,O,u_{1:t};\mathcolor{dark-green}{\BPsi})}{P(Y_{0:t}|u_{1:t};\mathcolor{dark-blue}{\BAlph})} \label{eq:marignal_mrf_2} 
\end{align}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{Figure7}
\caption{Filtered marginals. Illustration of the agent and object marginal update, Equation \ref{eq:marignal_mrf}. The joint 
distribution parameters which are independent $A \ominus O$ are pale and the dependent areas $A \cap O$, where $P(Y_t<1|A_t,O)$, are bright. MLMF only
evaluates the joint distribution in dependent states. For each state $s$ of the marginals $1,\dots,10$ the difference 
of the marginals inside the dependent area, before and after the measurement likelihood is applied, is evaluated and removed from the marginals 
$P(A_t|Y_{0:t-1},u_{1:t};\ThA)$, $P(O|Y_{0:t-1};\ThO)$ leading to $P(A_t|Y_{0:t},u_{1:t};\ThA)$, $P(O|Y_{0:t};\ThO)$ (we did not show $u_{1:t}$ in the figure for ease of notation). 
\textit{Bottom-left}: joint marginals $P(A_t|u_{1:t};\ThAs)$ and $P(O;\ThOs)$ remain unchanged by measurements.}
\label{fig:marginal_update}
\end{figure}

Equation \ref{eq:marignal_mrf} is recursive, $P(O|Y_{0:\mathbf{t}};\mathcolor{red}{\ThO})$ is computed in terms of $P(O|Y_{0:\mathbf{t-1}};\mathcolor{red}{\ThO})$. 
Figure \ref{fig:marginal_update} illustrates a measurement update of the MLMF.  The illustrated marginals (\textit{Bottom row}) are 
(on the \textbf{left}) the \textbf{joint marginals} $P(A_t|u_{1:t};\mathcolor{blue}{\ThAs})$, $P(O;\mathcolor{red}{\ThOs})$ and (on the \textbf{right}) the \textbf{filtered marginals} $P(A_t|Y_{0:t},u_{1:t};\mathcolor{blue}{\ThA})$, $P(O|Y_{0:t};\mathcolor{red}{\ThO})$. 


The shape of the \textbf{joint marginals} remain unchanged by measurements during the filtering process, they are the parameters of the joint distribution used to update the filtered marginals. 
Table \ref{tab:mlmf_parameters} summarises the functions and parameters of the MLMF for two random variables, an agent and object.

% parameters table

We evaluated the MLMF with Histogram-SLAM in the case of the 1D filtering scenario
illustrated in Figure \ref{fig:discrete_example} on page \pageref{fig:discrete_example} and we found them to be identical. Having respected the formulation of Bayes rule, we
assert that the MLMF filtering steps (see  Algorithm \ref{alg:mrf-slam}, Appendix \ref{app:alg:mlmf} for a more detailed application of motion-measurement update steps) are 
Bayesian Optimal Filter\footnote{An optimal Bayesian solution is an exact solution to the recursive problem of calculating the exact posterior density \cite{PF_tutorial_2002}}. 
Next we evaluate both space and time complexity of the MLMF filter.

\section{Space \& time complexity}

For discussion purposes we consider the case of three beliefs, namely that of the agent and two other objects $O^{(1)}$ and $O^{(2)}$, we
subsequently generalise. As stated previously $M$ stands for the number of filtered random variables including the agent and
$N$ is the number of discrete states in the world. In the following section, we compare the space and time complexity 
of MLMF-SLAM with Histogram-SLAM.


\subsection{Space complexity}

Figure \ref{fig:3bel_lik_profile} \textit{Left} illustrates the volume occupied by the joint distribution
for a space with $N$ states. Histogram-SLAM would require $N^3$ parameters for the joint distribution 
$P(A,O^{(1)},O^{(2)};\boldsymbol{\theta})$ and $3\,N$ parameters to store the marginals. In general 
for $M$ random variables $N^{M} + M\, N$ parameters are necessary, give a space complexity of $\BigO(N^M)$. 

For MLMF-SLAM, each random variable requires two sets of parameters, $\boldsymbol{\theta}$ and $\boldsymbol{\theta}^*$ 
(see Table \ref{tab:mlmf_parameters}). Given
$M$ random variables, the initial number of parameters is $M (2 N)$.
At every time step the likelihood memory function increments by one measurement and offset, $(Y_t,l=0)$ (Algorithm \ref{alg:motion_memory}).
Given a state space of size $N$, there can be no more than $N$ different measurement functions (one for each state). In
the worst case scenario the number of memory likelihood function parameters $\Psi_{0:t}$, Equation \ref{eq:memory}, will be $N$.
The total number of parameters is $M (2 N) + N$ which gives a final worst case space complexity linear in the number of 
random variables, $\BigO(M\,N)$. 

\begin{figure}
 \centering
  \includegraphics[width=0.8\textwidth]{Figure8}
  \caption{\textit{Left:} \textbf{Joint distribution} $P(A,O^{(1)},O^{(2)})$ of the agent and two objects ($Y_{0:t}$ and $u_{1:t}$ omitted). Each 
  likelihood function, $P(Y|A,O^{(1)})$, $P(Y|A,O^{(2)})$ corresponds to a hyperplane in the joint distribution
  The state space is discretised to $N$ bins giving a potential total of $N^3$  parameters for the joint distribution (Histogram case). 
  \textit{Right:} \textbf{Scalable-MLMF} Each agent-object joint distribution pair is modelled independently. For clarity we have left 
  out the action random variable $u$ which is linked to every agent node.
  Two joint distributions $P(A^{(1)},O^{(1)}|Y^{(1)}_{0:t})$   and $P(A^{(2)},O^{(2)}|Y^{(2)}_{0:t})$ parametrise the graphical model. 
  The dashed undirected lines represent a wanted dependency, if present $O^{(1)}$ and $O^{(2)}$ are to be dependent through $A$. In
  the standard setting there will be no exchange of information between the individual joint distributions. However we demonstrate later on how
  we perform a one time transfer of information when one of the objects is sensed.}
  \label{fig:3bel_lik_profile}
\end{figure}

\subsection{Time complexity}

For Histogram-SLAM, the computational cost is equivalent to that of the space complexity, $\BigO(N^M)$,
since every state in the joint distribution has to be summed to obtain all the marginals.

For MLMF-SLAM, every state in the joint distribution's state space which has been changed by the likelihood function 
has to be summed, see Figure \ref{fig:marginal_update} on page \pageref{fig:marginal_update}. As a result the computational complexity is directly 
related to the number of dependent states $|A \cap O|$. In Figure \ref{fig:marginal_update}, this corresponds to states where $i = j$ and there are $N$ out 
of a total $N^2$ states for that joint distribution. Figure \ref{fig:3bel_lik_profile} (\textit{Left})
illustrates a joint distribution with $N^3$ states. The dependent states $|A \cap O^{(1)} \cap O^{(2)}|$ are those which 
are within the blue and red planes (where the likelihood evaluates to zero) and comprise $N^2$ states each, 
giving a total of $2\,N^2 - N$ dependent states (negative is to remove the states we count twice at the intersection of the blue and red plane). 

The likelihood term $P(Y_t|A_t,O^{(1)})$ evaluates states to zero which satisfy ${(i=j,\forall k)}$, as 
the measurement of object $O^{(1)}$ is independent of object $O^{(2)}$. With 3 objects, the joint distribution would be
${P(A_t=i,O^{(1)}=j,O^{(2)}=k,O^{(l)}=l)}$ then the likelihood $P(Y_t|A_t,O^{(1)})$  evaluated to 
zero for ${(i=j,\forall k,\forall l)}$ which would mean $N^3$ dependent states.
In general, for $M$ random variables the computational cost is $(M-1) N^{M-1}$ which gives $\BigO(N^{M-1})$ as opposed to the Histogram-SLAM's $\BigO(N^M)$. 
The computation complexity in this setup is still exponential but to the order $M-1$ as opposed to $M$ which nevertheless 
quickly limits the scalability as more objects are added. 

Computing the value of a dependent state ${(i,j,k)}$ in the joint distribution required evaluating Equation \ref{eq:mlmf_filter_conditional} which
contains a product of $N$ likelihood functions, in the worst case scenario. However the likelihood functions are not overlapping and binary. As a result the complete product
does not have to be evaluated since only one likelihood function will effect the state ${(i,j,k)}$. Thus evaluating Equation \ref{eq:mlmf_filter_conditional}
yields a cost of $\BigO(1)$ and \textbf{not} $\BigO(N)$.

\section{Scalable extension to multiple objects}\label{subsec:scalabe_extension}

To make the MLMF filter scalable we introduce an \textbf{independence assumption} between the objects and model 
the joint distribution (Equation \ref{eq:pair_wise_joint}) as a product of agent-object joint distributions:

\begin{equation}\label{eq:pair_wise_joint}
 P(A_t,O^{(1)},\cdots,O^{(M-1)}|Y_{0:t},u_{1:t}) = \prod\limits_{i=1}^{M-1} P(A^{(i)}_t,O^{(i)}|Y^{(i)}_{0:t},u_{1:t})
\end{equation}

The measurement variable $Y_t$, is the vector of all agent-object 
measurements, $Y_t = \left[Y^{(1)}_t,\dots,Y^{(M-1)}_t\right]^{\mathrm{T}}$. Each agent-object joint distribution has its own parametrisation of the agent's marginal,
$A^{(1)}_t,\dots,A^{(M-1)}_t$ which combine to give the overall marginal of the agent $A_t$. The computation of each object marginal $P(O^{(i)}|Y^{(i)}_{0:t})$ is independent of the other objects. This is evident from the marginalisation 
see Equation \ref{eq:marg_indep}-\ref{eq:marg_indep_prod}.

\begin{align}
 P(O^{(i)}|Y^{(i)}_{0:t},u_{1:t}) &= \sum\limits_{A^{(i)}_t} P(A^{(i)}_t,O^{(i)}|Y^{(i)}_{0:t},u_{1:t}) \label{eq:marg_indep} \\
 P(A_t|Y_{0:t},u_{1:t})   &=\prod\limits_{i=1}^{M-1} \ P(A^{(i)}_t|Y^{(i)}_{0:t},u_{1:t}) \label{eq:marg_indep_prod}  
\end{align}

The independence assumption will create an unwanted effect with respect to agent's marginal $P(A_t|Y_{0:t},u_{1:t})$. 
At initialisation the agent marginals should be equal, $P(A_0|Y_0) = P(A^{(i)}_0|Y^{(i)}_0) \forall_i$, however this is not the case because of 
Equation \ref{eq:marg_indep_prod}. To overcome this we define the marginal, $P(A_t|Y_{0:t},u_{1:t})$, of the agent as being the average of all the individual
pairs $P(A^{(i)}|Y^{(i)}_{0:t},u_{1:t})$.

\begin{equation}
  P(A_t|Y_{0:t},u_{1:t}) := \frac{1}{M-1} \sum\limits_{i=1}^{M-1} \ P(A^{(i)}_t|Y^{(i)}_{0:t},u_{1:t}) \label{eq:marg_indep_sum}
\end{equation}

Figure \ref{fig:3bel_lik_profile} (\textit{Right}), depicts the graphical model of the scalable formulation. 
As each joint distribution pair has its own parametrisation of the agent's marginal and these do not subsequently get updated by one another,
the information gained by one joint distribution pair is \textbf{not transferred}.
A solution is to transfer information between the marginals $A^{(i)}$ at specific intervals namely when one of the objects is sensed by the agent. 

The exchange of information of one joint distribution to another is achieved through the agent's marginals $A^{(i)}$ according to Algorithm \ref{alg:scalabe-mrf-slam},  Appendix \ref{app:scalable-mlmf}.
The measurement update is the same as previously described in Algorithm \ref{alg:mrf-slam}  in the case of no positive measurements of the objects. If the agent
senses an object, all of the agent marginals of the remaining joint distributions are set to the marginal of the joint distribution pair belonging to the positive 
measurement $Y^{(i)}_t$. 

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{Figure9}
  \caption{\textbf{Transfer of information (joint distributions)} 
  \textit{Top:}  Joint distributions of $P(A^{(1)}_t,O^{(1)}|Y^{(1)})$ and $P(A^{(2)}_t,O^{(2)}|Y^{(2)})$ prior sensing, $Y_t^{(2)}=1$, see Figure \ref{fig:independence_object}
  (\textit{Top right}) for the corresponding marginals. The red and green lines across the joint distributions 
   correspond to the region in which the likelihood functions $P(Y^{(1)}_{t}|A^{(1)}_t,O^{(1)})$ and $P(Y^{(2)}_{t}|A^{(2)}_t,O^{(2)})$ will change the joint distributions.
  The dotted blue lines are to ease the comparison ofthe joint distributions prior and post sensing.
  \textit{Bottom right:}  After the agent has sensed $O^{(2)}$, all the probability mass which was not overlapping the green line becomes an infeasible
  solution to the agent and object locations. At this point the marginals $P(A^{(1)}_t|u_{1:t}) \not= P(A^{(2)}_t|u_{1:t})$ are no longer equal 
  (see the blue marginals \textit{Top}). \textit{Bottom left:} The constraint imposed by the likelihood function of the second object
  (green line) is transferred to the joint distribution of the first object according to Algorithm \ref{alg:scalabe-mrf-slam}.
  This results in a change in the joint distribution  $P(A^{(1)}_t,O^{(1)}|Y^{(1)})$, which satisfies the constraints 
  imposed by the agent's marginal from the joint distribution $P(A^{(2)}_t,O^{(2)}|Y^{(2)})$.}
  \label{fig:transfer_information}
\end{figure}

Figure \ref{fig:transfer_information}, depicts the process of information exchange between object $O^{(1)}$ and $O^{(2)}$ in the event that the agent 
senses $O^{(2)}$. Prior to the positive detection, both marginals $P(A^{(1)}_t|Y^{(1)}_{0:t-1},u_{1:t})$ and $P(A^{(2)}_t|Y^{(2)}_{0:t-1},u_{1:t})$ 
occupy the same region and are identical. When the agent senses $O^{(2)}$ the line defined by the measurement 
likelihood function $P(Y^{(2)}_t|A^{(2)}_t,O^{(2)})$ becomes a hard constraint implying that both the agent and $O^{(2)}$ have to satisfy this constraint.
Figure \ref{fig:independence_object} shows marginals at initialisation, prior contact between the agent and object and the after the measurement 
(post contact) has been integrated into the marginals (resulting from the joint distributions in Figure \ref{fig:transfer_information}).
%The marginals in the \textit{Left} plot are the result after updating the marginals $A^{(i)}$. The \textit{Right} plot shows the result for the case where the objects
%remain independent. 

The result of introducing a dependency between the objects through the agent's marginals in the event of a sensing and treating them
independently gives the same solution as the histogram filter in this particular case. However as each individual marginal $A^{(i)}_t$ diverges 
from the other marginals, the filtered solution will diverge from the histogram's solution. We assume however that the objects are weakly 
dependent and sharing information during positive sensing events is sufficient. In section \ref{subsec:eval_indep_assumptiom} we will 
evaluate the independence assumption with respect to the histogram filter.

Table \ref{tab:time_space_summary} summarises the time and space complexity for the three filters.% In the case of transfer of
%information between marginals the computational complexity is higher, however this is a one time occurrence. 

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{Figure10}
  \caption{\textbf{Transfer of information (marginals)} \textit{Top left:} Initial beliefs of the agent and object's location. The agent moves to the left until it senses object $O^{(2)}$.
  \textit{Top right:} Marginals prior the agent entering in contact with the green object, see Figure \ref{fig:transfer_information} (\textit{Top}) for an illustrate of the joint distributions.
  \textit{Bottom left:} resulting marginals after setting the agent marginals of each 
  joint distribution equal $A^{(1)}_t = A^{(2)}_t$ according to Algorithm \ref{alg:scalabe-mrf-slam}. The object marginal $P(O^{(2)}|Y_{0:t})$ is recomputed. 
  \textit{Bottom Right:} resulting marginals in which the objects have no influence on one another.
  Note that a transfer of information has caused a change in the marginal $O^{(1)}$.}
  \label{fig:independence_object}
\end{figure}

% Table space time measurement

\section{Evaluation}

We conduct three different types of evaluation to quantify the scalability and correctness of the scalable-MLMF filter. The first experiment
tests the scalability of our filter in terms of processing time taken per motion-measurement update cycle. The second experiment evaluates the independence 
assumption made in the scalable-MLMF filter between the objects. The third and final experiment determines the effect of the 
memory size on a search policy to locate all the objects in the \textit{Table} world.

\subsection{Evaluation of time complexity}

We measured the time taken by the motion-measurement update loop, as a function of the number beliefs and number of states per belief. 
We started with a 100 states per belief and gradually increase it to 10'000'000 over 50 steps. Each of the 50 steps treated 2 to 25  objects. 
Figure \ref{fig:time_complexity} \textit{left} illustrates the computational
cost as a function of number of states and objects. For each state-object pair 100 motion-measurement updates were performed. Most of the trials returned time updates 
below 1 Hz. Figure \ref{fig:time_complexity} \textit{right} shows the computational cost as a function of the number of states plotted for 6 different filter runs with
a different number of objects. As the number of states increases exponentially so does the computational cost. Note the cost increases at the same
rate as the number of states meaning that the computational complexity is linear with respect to the number of states. This result is in agreement with 
the asymptotic time complexity.

\begin{figure}
\centering
 \includegraphics[width=0.9\textwidth]{Figure11}
 \caption{\textbf{Time complexity:} \textit{left:} mean time taken for a loop update (motion and measurement) as a function of the number of states in a marginal and the 
 number of objects present. \textit{right:} time taken for a loop update with respect to the number of states in the marginal. The colour coded lines are 
 associated with the number of objects present. The computational cost is plotted on a log scale. As the number of states increases exponentially the
 computational cost matches it.}
 \label{fig:time_complexity}
\end{figure}

\subsection{Evaluation of the independence assumption}\label{subsec:eval_indep_assumptiom}

In section \ref{subsec:scalabe_extension} we made the assumption (for scalability reasons) that the objects' beliefs are independent
of one another. This assumption is validated by comparing the MLMF filter on three random variables, an agent and two objects, with the ground truth
which we obtain from the standard histogram filter. For each of the three beliefs (the agent and two objects), 100 different marginals 
were generated and the true locations (actual position of the agent and objects) were sampled. 
Figure \ref{fig:independence_assumption_test} \textit{Top-left} illustrates one instance of the initialisation of the agent and object marginals
with their associated sampled true position.
The agent carries out  a sweep of the state space for each of the marginals and the policy is saved 
and run with the scalable-MLMF filter. In the first experiment we assumed that the objects are completely independent 
and that there was no transfer of information between the pair-wise joint distributions. In the second and third experiments there 
is an exchange of information as described in Algorithm \ref{alg:scalabe-mrf-slam}. Here we compare the effect of using 
the product of the agent's marginals, Equation \ref{eq:marg_indep_prod}, with the average of the marginals, Equation \ref{eq:marg_indep_sum}.
We expect the average of the the agent's marginal to yield a result closer to the ground truth as the marginal of the 
agent $P(A_t|Y_{0:t},u_{1:t})$ at initialisation is the same as the ground truth (the Histogram-SLAM's). As for the marginal of the 
objects $P(O^{(i)}|Y_{0:t})$ we expect the difference between them to be independent of whether the product or average of the 
agent's marginal is used. This results from Algorithm \ref{alg:scalabe-mrf-slam}. When an object $i$ is sensed all the corresponding 
agent marginals $P(A^{(j)}|u_{1:t})$ are set equal to $P(A^{(i)}|u_{1:t})$ and not to $P(A_t|Y_{0:t},u_{1:t})$. This is a design 
decision of our information transfer heuristic. There are many other possibilities but this is one of the simplest.
For each of the 100 sweeps the ground truth is compared with the scalabe-MLMF using the Hellinger distance (Equation \ref{eq:hellinger})
\begin{equation} \label{eq:hellinger}
 H(P,Q) = \frac{1}{\sqrt{2}}\, \|\sqrt{P} - \sqrt{Q}\|_2  
\end{equation}
which is a metric which measures the distance between two probability distributions. Its value lies strictly between 0 (the two 
distributions are identical) and 1 (no overlap between them). Figure \ref{fig:independence_assumption_test} shows the kernel density 
distribution of the Hellinger distances taken at each time step for all 100 sweeps. In the \textit{Top-left} of the figure, 
for the case when no transfer of information is a applied, all the marginals are far from the 
ground truth. This results from the introduction of the independence assumption, necessary to scale the MLMF. 
Figure \ref{fig:independence_assumption_test} \textit{Bottom} shows the results for difference between the product and average of the agents
marginals. As expected there is no difference between the objects' marginals when considering both methods (product and average) with respect
to the ground truth. The predominant difference occurs in the agent's marginal $P(A_t|Y_{0:t},u_{1:t})$. This is also expected and 
prompted the introduction of the average method instead of the product. 

The scalable-MLMF information exchange heuristic will not lead to any of the objects marginals probability mass being falsely  
removed during the information transfer, which is close to a winner-take-all approach in terms of beliefs.
When object $i$ is sensed its associated agent marginal is set to all other agent-object joint pairs, which results in the 
information accumulated in the $j$th agent marginals being replaced by the $i$th.

\begin{figure}
\centering
 \includegraphics[width=0.8\textwidth]{Figure12}
 \caption{\textbf{Comparison of scalable-MLMF and the histogram filter} A deterministic sweep policy was carried out for 100 different initialisations of 
 the agent and object beliefs. \textit{Top left:} One particular Initialisation of the agent and object
 random variables. The true position of the agent and objects were sampled at random. The black arrow indicates the general policy which was 
 followed for each of the 100 sweeps. 
 These were performed for \textbf{1)} scalable-MLMF  with objects considered to be independent at all times (no Algorithm \ref{alg:scalabe-mrf-slam}). 
 \textbf{2)} Agent marginal $P(A_t|Y_{0:t},u_{1:t})$ is the product of marginals $P(A^{(i)}_t|Y^{(i)}_{0:t},u_{1:t})$, Equation \ref{eq:marg_indep_prod}. 
 \textbf{3)} marginal $P(A_t|Y_{0:t},u_{1:t})$ is taken to be the average of all marginals $P(A^{(i)}_t|Y^{(i)}_{0:t},u_{1:t})$, Equation \ref{eq:marg_indep_sum}.  For each of these three experiment we report the 
 kernel density estimation over the Hellinger distances taken at every time step between ground truth (from histogram filter) and scalable-MLMF.}
 \label{fig:independence_assumption_test}
\end{figure}

\subsection{Evaluation of memory}

The memory measurement likelihood function $P(Y_{0:t}|A_t,O,u_{1:t};\Psi_{0:t})$ is parameterised by the 
history of all the measurement likelihood functions which have been applied on the joint 
distribution since initialisation. As detailed previously there can be no more than $|\Psi_{0:t}| \leq N$ different measurement likelihood functions added to 
memory. In the case of a very large state space this might be cumbersome. We investigate how restricting the memory size, the number 
of parameters $|\Psi_{0:t}|$, can impact on the decision process in an Active-SLAM setting. Given our set up a breadth-first search in the action 
space is chosen with a one time step horizon, making it a greedy algorithm. The objective function utilised is the information
gain of the beliefs after applying an action, Equation \ref{eq:greedy_algorithm}.

\begin{equation}\label{eq:greedy_algorithm}
 u_{t} = \operatorname*{arg\,max}_{u_t} H\{P(A_{t-1},O|Y_{0:t-1},u_{1:t-1})\} - \mathbb{E}_{Y_t}\left[H\{P(A_{t},O|Y_{0:t},u_{1:t})\}\right]
\end{equation}

For each action the filter is run forward in time and all future measurements since we cannot know ahead of time the actual 
measurement. The information gain is the difference between the current entropy (defined 
by $H\{\cdot\}$) and the future entropy after the simulated motion and measurement update. The action with the highest information gain 
is subsequently selected. This is repeated at each time step. Figure \ref{fig:exploration_init} illustrates the environment setup for 
a 1D and 2D case. The agent's task is to find the objects in the environment.


\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{Figure13}
  \caption{\textbf{Agent's prior beliefs.} Two types of environment, the first is 
  a 2D world where the agent lives in a square surrounded by a wall whilst the second is a 1D
  world. In the 2D figures the agent is illustrated by a circle with a bar to indicate its heading. The true location 
  of the objects are represented by colour coded squares. \textit{Top row} three different initialisations of the agent's location. 
  \textit{Bottom row} d) the agent's prior beliefs with respect to the location of the first object and e) belief of the second object's location.
  \textit{bottom row} f) 1D world with one object.}
  \label{fig:exploration_init}
\end{figure}

For the 2D search we consider three different initialisations (single-Gaussian, four-Gaussian, Uniform) for the agent's belief where there are 
two objects to be found. Ten searches are carried out for each of the three initialisations of the agent's beliefs. 
The agent's true location, for each search, is sampled from its initial belief, and the objects' locations 
(red and green squares in Figure \ref{fig:exploration_init}) are kept fixed throughout all searches. Each search is repeated for 
18 different memory sizes ranging from 1 to $N$ (the number of states). For the 1D search case one object is considered since adding more objects  
makes the search easier and the interest lies in the memory effects of the search and not the search itself. In Figures \ref{fig:time_to_reach_goal_1D}-\ref{fig:time_to_reach_goal_2D} we 
report on the time taken to find all objects with respect to a given memory size which is shown as the percentage of the total number of states. 
In the 1D search case the time variability taken to find the object converges when the memory size is at 60\% of the original state space. 
As for the 2D search with 2 beliefs (agent \& 1 object) the convergence depends on the agent's initial belief. For the 1-Gaussian (green line) 
all searches take approximately the same amount of time after a memory size of 9\%. As for the remaining two initialisations convergence is achieved at  48\%. 
The same holds true for the case of 3 beliefs (agent \& 2 objects).

In the 2D searches, the memory size has a less impact on the time taken to find the objects than in the 1D (which is a special search case). 
Only when the memory size is less than 6\% is there a significant change. We conclude that at least in the case of 
the greedy one step-look ahead planner which is frequently used in the literature, the size of the memory seems not to be a limiting factor in terms of the time taken to accomplish the search.

\begin{figure}	
  \centering
  \includegraphics[width=0.5\textwidth]{Figure14}
  \caption{\textbf{Memory size vs time to find object in 1D} Results of the effect of the memory size on the decision process
  for the 1D search illustrated in Figure \ref{fig:exploration_init} \textit{f)}.
  The memory size is reported as the percentage of total number of states present in the marginal space. At 100\% the size
  of the memory is equal to that of the state space, $N=100$ in this case. A total sweep of the entire state space would result in a total of 
  200 steps, the dotted grey line in the above figure. When no restrictions are placed on the memory size the policy following the greedy 
  approach takes around 180 steps. This result converges when the number of parameters $|\Psi_{0:t}|$ of the memory likelihood function is 
  greater than 50\% of the original state space. } 
  \label{fig:time_to_reach_goal_1D}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{Figure15}
  \caption{\textbf{Memory size vs time to find objects in 2D}. The initial beliefs correspond 
  to those of Figure \ref{fig:exploration_init}, a) for Gaussian (green line), b) 4 Gaussians (red line)
  and c) Uniform (blue line), both objects are initialised according to d) and e).}
   \label{fig:time_to_reach_goal_2D}
\end{figure}

\section{Conclusion}

This work addresses the Active-SLAM filtering problem for scenarios in which sensory information relating to the map is very limited. Current
SLAM algorithms filter the errors originating from sensory measurements and not prior uncertainty. By making the assumption
that the joint distribution of all the random variables is a multivariate Gaussian, inference is tractable. Since the origin of 
the uncertainty does not originate from the measurement noise, no assumption can be made about the structure of the joint distribution.
In this case a suitable filter would be the histogram which makes no assumption about the shape or form taken by the joint distribution. 
However, the space and time complexity are exponential with respect to the number random variables and this is a major 
limiting factor for scalability. 

The main contribution of this work is a formulation of a histogram Bayesian state space estimator in which the computational complexity is 
both linear in time and space. A different approach to other SLAM formulations as been taken in the sense that
the joint distribution is not explicitly parameterised avoiding the exponential increase in parameter space which would otherwise have been the case. 
The MLMF parameters consist of the marginals and the history of measurement functions which have been applied. By solely evaluating the joint 
distribution at the states which are affected by the current measurement function whilst taking into account the 
memory, the MLMF filter obtains the same filtered marginals as the histogram filter. Further, the worst case space complexity is 
linear rather than exponential and the time complexity remains exponential but increases at lower rate than in the histogram filter.
In striving to make the filter scalable we make the assumption that the objects are independent. An individual MLMF
is used for each agent-object pair. We evaluate the difference between the scalable-MLMF with a ground truth provided 
by the histogram filter for 100 different searches with respect to the Hellinger distance. We conclude that 
the divergence is relatively small and thus the scalable-MLMF filter provides a good approximation to the true filtered
marginals. We evaluate the time taken to perform a motion-update loop for different discretisations of the state 
space (100 to 10'000'000 states) and number of objects (2 to 25). In most of the cases we achieve an update cycle rate below 1Hz. 
We evaluate how the increase of the number of states effects the computational cost and find  the relationship to be linear and thus 
in agreement with our analysis of the asymptotic growth rate. We analyse the effect of the memory size 
(the remembered number of measurement likelihood functions) on the decision theoretic process of reducing 
the uncertainty of the map and agent during a search task. 
We conclude that in the 2D case the memory size has much less effect than in the 1D case and that it 
is unnecessary to remember every single measurement function.

This implies that the MLMF and scalable-MLMF that we have are a computationally tractable means of 
performing SLAM in a case scenario in which mostly negative information is present and the 
joint distribution cannot be assumed to have any specific structure. Furthermore, the filter can be used at a higher cognitive level than 
the processing of raw sensory information as is often the case in Active-SLAM. MLMF would be well suited for reasoning tasks 
where the robot's field of view is limited.

An interesting future extension could be to make the original MLMF filter scalable without introducing assumptions.
One possibility could to be to consider Monte Carlo integration methods for inference. These can scale well to high dimensional 
spaces whilst still providing reliable estimates. A second possibility could be to investigate the use of Gaussian Mixtures as a 
form of parameterisation of the marginals to blend our filter with EKF-SLAM. This would allow the parameters 
to grow quadratically with respect to the dimension of the marginal space as opposed to exponentially as is the case 
with the histogram and MLMF filters.




\section{Appendix}


\subsection{Tables}

\begin{table}[h]
\centering
\fbox{
\begin{tabular}{lcll}
    functions             	&   & parameters                      & description \\\hline
  $P(A_t|Y_{0:t},u_{1:t})$      & : & $\boldsymbol{\theta}_a$	      & filtered marginals\\
  $P(O|Y_{0:t})$          	& : & $\boldsymbol{\theta}_o$	      &  \\
  $P(A_t|u_{1:t})$       	& : & $\boldsymbol{\theta}^*_a$       & joint marginals\\
  $P(O)$                  	& : & $\boldsymbol{\theta}^*_o$       & \\
  $P(Y_{0:t}|u_{1:t})$      	& : & $\alpha_{0:t} \in \mathbb{R}$         & evidence\\
  $P(Y_{0:t}|A_t,O,u_{1:t})$  	& : & $ \Psi_{0:t} = \{(Y_i,l_i)\}_{i=0:t}$ & likelihood history
\end{tabular}
}
\caption{MLMF functions with associated parameters. The marginal parameters are the discretisation of the 
state space $\boldsymbol{\theta} \in \mathbb{R}^N$, $\boldsymbol{\theta}^{(s)}$ correspond to the probability being in state $s$.}
\label{tab:mlmf_parameters}
\end{table}


\begin{table}
 \centering
 \begin{tabular}{c|c|c|}
\cline{2-3}
				        &    \textbf{space}   &     \textbf{time} \\ \hline
    \multicolumn{1}{|l}{Histogram}      & \multicolumn{1}{|l}{$\BigO(N^M)$}   &  \multicolumn{1}{|l|}{$\BigO(N^M)$}      \\ \hline
    \multicolumn{1}{|l}{MLMF}           & \multicolumn{1}{|l}{$\BigO(M\,N)$}  &  \multicolumn{1}{|l|}{$\BigO(N^{(M-1)})$} \\ \hline
    \multicolumn{1}{|l}{scalable-MLMF}  & \multicolumn{1}{|l}{$\BigO(M\,N)$}  &  \multicolumn{1}{|l|}{$\BigO(M\,N)$}     \\ \hline
   \end{tabular}
   \caption{\textbf{Time and space complexity summary} For both MLMF and scalabe-MLMF the worst case scenario is reported for the space complexity.}
   \label{tab:time_space_summary}
\end{table}


\subsection{MLMF Algorithm}\label{app:alg:mlmf}

\begin{center}
\begin{minipage}{\linewidth}
\begin{algorithm}[H]
\label{alg:mrf-slam}
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\Input{\\
       \textbf{measurements}\\
	$\mathbf{Y_t}$, $\mathbf{u_t}$\\
       \textbf{joint distribution parameters}:\\
       $P(A_{t-1}|u_{1:t-1})$ $P(O)$, $\Psi_{0:t-1}$, $\alpha_{0:t-1}$ \\
       \textbf{filtered marginals}:\\
       $P(A_{t-1}|Y_{0:t-1},u_{1:t-1})$, $P(O|Y_{0:t-1})$}
\Output{\\
       \textbf{joint parameters}:\\
        $P(A_t|u_{1:t})$, $\Psi_{0:t}$, $\alpha_{0:t}$\\
       \textbf{filtered marginals}:\\
	$P(A_t|Y_{0:t},u_{1:t})$, $P(O|Y_{0:t})$}
\nonl\hrulefill	
\BlankLine
\nonl\textbf{initialisation}\\
\vspace*{-0.5cm}
\nonl\begin{align}
P(A_0;\theta_a) &:= P(A_0;\theta^*_a) \hspace*{6.5cm} \nonumber \\
P(O;\theta_o) &:= P(O;\theta^*_a)\nonumber \\
\Psi_0 &:= \{\}\nonumber\\
\alpha_0 &:= 0 \nonumber
\end{align}
\vspace*{-1cm}
\BlankLine
\nonl\hrulefill	\\
\nonl\textbf{motion update}\\
\vspace*{-0.5cm}
\nonl\begin{align}
P(A_t|u_{1:t})  	 &= \sum\limits_{A_{t-1}} P(A_t|A_{t-1},\mathbf{u_t})  P(A_{t-1}|u_{1:t-1}) \nonumber \\
P(A_t|Y_{0:t-1},u_{1:t}) &= \sum\limits_{A_{t-1}} P(A_t|A_{t-1},\mathbf{u_t})  P(A_{t-1}|Y_{0:t-1},u_{1:t-1}) \nonumber \\
\bar{\Psi}_{0:t} 	 &\gets \Psi_{0:t-1}: \mathrm{ Algorithm\;\; \ref{alg:motion_memory}\;\; \textit{(motion update)}} \nonumber
\end{align}
\vspace*{-1cm}
\BlankLine
\nonl\hrulefill	\\
\nonl\textbf{measurement update}
\nonl\begin{align}
 &\alpha_{0:t}       = \alpha_{0:t-1} + \sum\limits_{A_t}\sum\limits_{O} \Big(P(\mathbf{Y_t}|A_t,O) - 1\Big)P_{\cap}(A_t,O,Y_{0:t-1}|u_{1:t}) \nonumber \\
 &P(Y_{0:t}|u_{1:t}) = 1 + \alpha_{0:t} \nonumber\\
 &P(A_t|Y_{0:t})     = P(A_t|Y_{0:t-1}) - \Big(P_{\cap}(A_t|Y_{0:t-1}) -  P_{\cap}(A_t|Y_{0:\mathbf{t}}) \Big) \nonumber \\
 &P(O|Y_{0:t})       = P(O|Y_{0:t-1}) -  \Big(  P_{\cap}(O_t|Y_{0:t-1}) -  P_{\cap}(O_t|Y_{0:\mathbf{t}})\Big) \nonumber \\    
 &\Psi_{0:t} \gets \bar{\Psi}_{0:t}: \mathrm{Algorithm\;\; \ref{alg:motion_memory}\;\; \textit{(measurement update)}} \nonumber
 \end{align}
\caption{MLMF-SLAM}
\end{algorithm} 
\end{minipage}
\end{center}

\subsection{Scalabe-MLMF Algorithm}\label{app:scalable-mlmf}

\begin{center}
\begin{minipage}{\linewidth}

\begin{algorithm}[H]
\label{alg:scalabe-mrf-slam}
\SetKwComment{Comment}{$\triangleright$\ }{}
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\Input{$P(A^{(i)}_t|u_{1:t})$, $P(A^{(i)}_t|Y^{(i)}_{0:t-1},u_{1:t})$\\$P(O^{(i)})$, $P(O^{(i)}|Y^{(i)}_{0:t-1},u_{1:t})$\\ $Y^{(i)}_t$\\$i=1,\cdots,M$}
\BlankLine

\Comment{If object $i$ has been sensed by the agent}
\eIf{$Y^{(i)}_t == 1$}{
% \Comment*[r]{measurement update Algo. \ref{alg:mrf-slam} }
$P(O^{(i)}|Y^{(i)}_{0:t})   \gets P(O^{(i)}|Y^{(i)}_{0:t-1})$  \Comment*[r]{measurement update Algo. \ref{alg:mrf-slam}} 
$P(A^{(i)}_t|Y^{(i)}_{0:t},u_{1:t}) \gets P(A^{(i)}_t|Y^{(i)}_{0:t-1},u_{1:t})$\\


\ForAll{$j\in(1,\dots M-1) \setminus i$}
{
$P(A^{(j)}_t|Y_{0:t},u_{1:t}) = P(A^{(i)}_t|Y_{0:t},u_{1:t})$ \\
$P(A^{(j)}_t|u_{1:t}) = P(A^{(i)}_t|u_{1:t})$\\
$P(O^{(j)}|Y^{(i)}_{0:t}) \leftarrow \sum\limits_{A^{(j)}} P(A^{(j)}_t,O^{(j)}|Y^{(i)}_{0:t})$
}
}{

\ForAll{$i\in(1,\dots M)$}{
  measurement update Algo. \ref{alg:mrf-slam}
}
}
\caption{Scalable-MLMF: Measurement Update}
\end{algorithm} 
\end{minipage}
\end{center}

\subsection{Recursion example}\label{appendix:recursion_example}

Derivation of the filtered joint distribution, $P(A_t,O,Y_t|Y_{0:t},u_{1:t})$, for 
two updates. At initialisation when no action has yet been taken the filtered joint distribution 
is the product of the initial marginals and first likelihood function:
\begin{equation}
  P(A_0,O,Y_0) = P(O) P(A_0) P(Y_0|A_0,O) 
\end{equation}
  
The a first action, $u_1$ is applied, which to get the filtered joint distribution is marginalised:
\begin{align}
  P(A_1,O,Y_0|u_1) &= P(O)\sum\limits_{A_0} P(A_1|A_0,u_1) P(A_0) P(Y_0|A_0,O)\\
                   &= P(O)\sum\limits_{A_0} P(A_1,A_0,Y_0|u_1,O)\\
                   &= P(O) P(A_1,Y_0|u_1,O) \label{eq:ch5:rec_ex1}\\
                   &= P(O) P(Y_0|A_1,O,u_1) P(A_1|u_1,\Ccancel[red]{O}) \label{eq:ch5:rec_ex2}\\
                   &= P(O) P(Y_0|A_1,O,u_1) P(A_1|u_1) 
\end{align} 
From Equation \ref{eq:ch5:rec_ex1} to \ref{eq:ch5:rec_ex2} we used the Chain rule and 
the cancellation in Equation \ref{eq:ch5:rec_ex2} arise from the factorisation of the joint distribution, 
see Figure \ref{fig:bayesian_sse_dag} on page \pageref{fig:bayesian_sse_dag}, $A$'s marginal does not depend on $O$.
After the application of the first action, the filtered joint has the following form:
\begin{equation}
 P(A_1,O,Y_0|u_1) = P(O) P(A_1|u_1) P(Y_0|A_1,O,u_1)
\end{equation}
A second measurement $Y_1$ and action $u_2$ are integrated into the filtered joint distribution:
\begin{align}
 P(A_2,O,Y_{0:1}|u_{1:2}) &= P(O) \sum\limits_{A_1} P(A_2|A_1,u_2) P(A_1|u_1) P(Y_0|A_1,O,u_1) P(Y_1|A_1,O) \nonumber \\
 &=P(O) \sum\limits_{A_1} P(A_2,A_1|u_{1:2}) P(Y_{0:1}|A_1,O,u_1) \nonumber \\
 &=P(O) \sum\limits_{A_1} P(A_2,A_1,Y_{0:1}|O,u_{1:2})  \nonumber \\
 &=P(O) P(A_2,Y_{0:1}|O,u_{1:2})  \\
 &=P(O) P(Y_{0:1}|A_2,O,u_{1:2})P(A_2|\Ccancel[red]{O},u_{1:2}) \label{eq:ch5:rec_ex3} 
\end{align}
We expand the function $P(Y_{0:1}|A_2,O,u_{1:2})$ to give a sense of how the likelihood function's positions 
get as illustrated in Figure \ref{fig:discrete_example} on page \pageref{fig:discrete_example}.

\begin{align}
  P(Y_0,Y_1|A_2,O,u_1,u_2) &= P(Y_0|\Ccancel[red]{Y_1},A_2,O,u_1,u_2) P(Y_1|A_2,O,\Ccancel[red]{u_1},u_2) \\
		           &= P(Y_0|A_2,O,u_{1:2}) P(Y_1|A_2,O,u_2)
\end{align}
The first likelihood of measurement $Y_0$ is dependent on the last to applied actions whilst the likelihood
of $Y_1$ is dependent on the last action.
 
Repeating the above for $Y_{2:t}$ and $u_{3:t}$ results in:
\begin{equation}
 P(A_t,O,Y_{0:t}|u_{1:t}) = P(O)P(A_t|u_{1:t}) \prod_{i=0}^{t} P(Y_i|A_t,O,u_{i+1:t})
\end{equation}
If $t=3$, ($Y_{0:3}$ and $u_{1:3}$) according to the above equation we would get:
\begin{align}
 P(A_3,O,Y_{0:3}|u_{1:3}) = P(O)P(A_3|u_{1:3}) &P(Y_0|A_3,O,u_{1:3}) \nonumber \\
					       &P(Y_1|A_3,O,u_{2:3}) \nonumber \\
					       &P(Y_2|A_3,O,u_{3:3}) \nonumber \\
					       &P(Y_3|A_3,O,\Ccancel[red]{u_{4:3}})
\end{align}
We introduce some notation rules, first if $(i+1) > t$ for $u_{(i+1):t}$ then it cancels out since the current
measurement $Y_t$ cannot depend on a future action $u_{(i+1)}$.

\subsection{Derivation of the evidence}\label{appendix:evidence}

The evidence, also known as the marginal likelihood, is the marginalisation of all non measurement random variables from 
the filtered joint distribution $P(A_t,O,Y_{0:t}|u_{1:t})$. We detail below how we compute the evidence in a recursive 
manner whilst only considering dependent regions of the joint distribution.

We start with the \textbf{standard} definition of the evidence:
\begin{equation}\label{eq:ch5:numerator}
 P(Y_{0:t}|u_{1:t}) = \sum\limits_{A_t}\sum\limits_{O} P(A_t,O,Y_{0:t}|u_{1:t}) \in \mathbb{R}
\end{equation}
If both $A_t$ and $O$ are random variables defined over a discretised state space of $N$ states,
the above double integral will sum a total of $N^2$ states which is the complete state space of the 
joint distribution $P(A_t,O,Y_{0:t}|u_{1:t}) \propto  P(A_t,O|Y_{0:t},u_{1:t})$, see Figure \ref{fig:maringal_joint_example_v2} 
on page \pageref{fig:maringal_joint_example_v2} for an illustrate of such a joint distribution.
As we are interested in a recursive computation of the evidence, we consider the gradient:
\begin{align}
  \alpha_t = \nabla_{Y_t}P(Y_{0:t}|u_{1:t}) =  P(Y_{0:t}|u_{1:t}) - P(Y_{0:t-1}|u_{1:t})
\end{align}

\begin{align}
 \alpha_t  &= \sum\limits_{A_t}\sum\limits_{O} P(A_t,O,Y_{0:t}|u_{1:t}) - P(A_t,O,Y_{0:t-1}|u_{1:t})  \\
	   &= \sum\limits_{A_t}\sum\limits_{O} P(Y_t|A_t,O)P(A_t,O,Y_{0:t-1}|u_{1:t}) - P(A_t,O,Y_{0:t-1}|u_{1:t}) \\
	   &= \sum\limits_{A_t}\sum\limits_{O} (P(Y_t|A_t,O) - 1)P(A_t,O,Y_{0:t-1}|u_{1:t}) \label{eq:ch5:gradient_alpha}
\end{align}
The gradient $\alpha_t$ is the difference in mass before and after the application the likelihood function, $P(Y_t|A_t,O)$. The above 
summation, Equation \ref{eq:ch5:gradient_alpha}, is over the entire joint distribution state space. We can take advantage of the fact 
that the likelihood function is sparse and will only affect a small region of the joint distribution, which we called the dependent states, $\cap$.
The states which are not affected by the joint distribution will result in a contribution of zero to Equation \ref{eq:ch5:gradient_alpha}. We 
rewrite the gradient update in terms of only the dependent regions:

\begin{equation}
 \alpha_t = \sum\limits_{A_t}\sum\limits_{O} (P(Y_t|A_t,O) - 1)P_{\cap}(A_t,O,Y_{0:t-1}|u_{1:t})
\end{equation}

Consider the first update of the evidence at time $t=0$:

\begin{align}
 \alpha_0 &= \sum\limits_{A_t}\sum\limits_{O} (P(Y_0|A_0,O) - 1)P(A_0,O) 
\end{align}
The one in Equation \ref{eq:ch5:evidence_yo} is the original value of the normalisation denominator before any observation is made and as the 
initial joint distribution $P(A_0,O)$ is normalised the value of the denominator is one.
\begin{equation}
 P(Y_0) = 1 + \alpha_0 \label{eq:ch5:evidence_yo}
\end{equation}
For the evidence $P(Y_{0:t}|u_{1:t})$ we consider the summation of all the derivatives $\alpha_t$ from time $t=0$ until $t$:
\begin{equation}
 P(Y_{0:t}|u_{1:t}) = 1 + \sum_{t=0}^t \alpha_t \label{eq:ch5:evidence_y}
\end{equation}

\subsection{Derivation of the marginal}\label{appendix:marginal}

The marginal of a random variable is the marginalisation or integration over all other random variables, $P(A_t,|Y_{0:t}) = \sum\limits_{O} P(A_t,O|Y_{0:t})$. Below 
we give a form of this integration which exploits the independent regions in the joint distribution.

\begin{equation} \label{eq:ch5:app:dummy}
 P(A_t,|Y_{0:t}) = \mathbf{P(A_t|Y_{0:t-1})} - \Big(\mathbf{P(A_t|Y_{0:t-1})} - P(A_t|Y_{0:t}) \Big) 
\end{equation}

In Equation \ref{eq:ch5:app:dummy} we add and substract $P(A_t|Y_{0:t-1})$ and we further split 
$P(A_t|Y_{0:t-1})$ into independent and dependent components: 

\begin{align}
  &P(A_t,|Y_{0:t}) =  P(A_t|Y_{0:t-1}) - \nonumber \\ 
  &\Big(\underbrace{P_{\cap}(A_t|Y_{0:t-1}) + \Ccancel[red]{P_{\ominus}(A_t|Y_{0:t-1})}}_{P(A_t|Y_{0:t-1})} -  \underbrace{P_{\cap}(A_t|Y_{0:t}) + \Ccancel[red]{P_{\ominus}(A_t|Y_{0:t})}}_{P(A_t|Y_{0:t})})   \Big) \label{eq:dependence1} 
\end{align}
From equation \ref{eq:dependence1} to \ref{eq:dependence2} we used the fact that independent regions of the marginal distributions will remain unchanged after
an observation, $P_{\ominus}(A_t|Y_{0:t-1}) = P_{\ominus}(A_t|Y_{0:t})$, and before re-normalisation. This results in the final recursive update:
\begin{equation}
 P(A_t,|Y_{0:\color{red}{t}}) =  P(A_t|Y_{0:\color{red}{t-1}}) - \Big(P_{\cap}(A_t|Y_{0:t-1}) -  P_{\cap}(A_t|Y_{0:t})  \Big) \label{eq:dependence2} 
\end{equation}
Equation \ref{eq:dependence2} states that only elements of the marginals which are dependent will change by the difference
before and after a measurement update.



\bibliographystyle{frontiersinSCNS_ENG_HUMS} % for Science, Engineering and Humanities and Social Sciences articles, for Humanities and Social Sciences articles please include page numbers in the in-text citations
\bibliography{bib/MLMF.bib}


\end{document}
