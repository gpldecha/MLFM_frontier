\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Plagemann07gaussianbeam}
\citation{DataAssociation2003}
\newcplabel{^_1}{1}
\citation{SLAM_part1}
\citation{TutGraphSLAM}
\citation{FastSLAM}
\citation{Thrun_Burgard_Fox_2005}
\citation{SLAM_HBR}
\citation{Thrun_grid_based_1996}
\citation{Kollar_2008_Exploration_SLAM}
\citation{Navigation_strategires_for_exploring_indoor_environments}
\citation{PRM_1996}
\citation{RRT-SLAM}
\citation{ActivePosSLAM}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{1.1}}Active-SLAM \& Exploration}{2}{subsection.1.1}}
\citation{stachniss05robotics}
\citation{Ross08onlineplanning}
\citation{GeorgiosLidoris}
\citation{Active_SLAM_Uncertainty_compar,KL_SLAM_exploration_PF}
\citation{Thrun_Burgard_Fox_2005}
\citation{Thrun02particlefilters,negative_info_markov_localisation}
\@writefile{toc}{\contentsline {subsection}{\numberline {{1.2}}Problem Statement}{3}{subsection.1.2}}
\citation{NegInfoFurtherStudies}
\citation{Bake_Saxe_Tene_2011}
\citation{deChambrier2013}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Table World} There are three different probability density functions present on the table. The blue represents the believed location of the agent, the red and green probability distributions are associated with object 1 and 2. The white shapes in each figure represent the true location of each associated object or agent.\relax }}{4}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Figure1}{{1}{4}{\textbf {Table World} There are three different probability density functions present on the table. The blue represents the believed location of the agent, the red and green probability distributions are associated with object 1 and 2. The white shapes in each figure represent the true location of each associated object or agent.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{1.3}}The main contribution to the field of Artificial Intelligence}{4}{subsection.1.3}}
\citation{BayesBall}
\@writefile{toc}{\contentsline {section}{\numberline {2}Bayesian State Space Estimation}{5}{section.2}}
\newlabel{sec:BSSE}{{2}{5}{Bayesian State Space Estimation}{section.2}{}}
\citation{SLAM_part1}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Directed graphical model of dependencies between the agent(A) and object(O)'s estimated location. Each object, $O^{(i)}$ is associated with one sensing random variable $Y^{(i)}$. The overall sensing random variable is $Y = \left [Y^{(1)},\dots  ,Y^{(M-1)}\right ]^{\mathrm  {T}}$, where $M$ is the total number of agent and object random variables in the filter. For readability we have left out the time index $t$ from $A$ and $Y$. Since the objects are static, they have no temporal process associated with them thus they will never have a time subscript. The two models necessary for filtering are the motion model $P(A_t|A_{t-1},u_t)$ (red) and measurement model $P(Y_t|A_t,O)$ (blue).\relax }}{6}{figure.caption.2}}
\newlabel{fig:bayesian_sse_dag}{{2}{6}{Directed graphical model of dependencies between the agent(A) and object(O)'s estimated location. Each object, $O^{(i)}$ is associated with one sensing random variable $Y^{(i)}$. The overall sensing random variable is $Y = \left [Y^{(1)},\dots ,Y^{(M-1)}\right ]^{\mathrm {T}}$, where $M$ is the total number of agent and object random variables in the filter. For readability we have left out the time index $t$ from $A$ and $Y$. Since the objects are static, they have no temporal process associated with them thus they will never have a time subscript. The two models necessary for filtering are the motion model $P(A_t|A_{t-1},u_t)$ (red) and measurement model $P(Y_t|A_t,O)$ (blue).\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{2.1}}EKF-SLAM}{6}{subsection.2.1}}
\newlabel{sec:EKF-SLAM}{{{2.1}}{6}{EKF-SLAM}{subsection.2.1}{}}
\newlabel{eq:lik-measurement}{{2}{6}{EKF-SLAM}{equation.2.2}{}}
\newlabel{eq:measurement_ekf}{{3}{6}{EKF-SLAM}{equation.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {a)} EKF-SLAM time slice evolutions of the pdfs. The temporal ordering is given by the numbers in the top right corner of each plot. The blue pdf represents the agent's believed location and the circle is the agent's true location. The same holds for the red distribution which represents the agent's belief of the location of an object. \textbf  {b)} Evolution of the covariance components of $\Sigma $ over time and true $Y_t$ and expected measurements, $\mathaccentV {hat}05E{Y}_t$. $\Sigma _a$ and $\Sigma _o$ are the variances of the agent and object positions and $\Sigma _{ao}$ is the cross-covariance term.\relax }}{7}{figure.caption.3}}
\newlabel{fig:EKF-SLAM}{{3}{7}{\textbf {a)} EKF-SLAM time slice evolutions of the pdfs. The temporal ordering is given by the numbers in the top right corner of each plot. The blue pdf represents the agent's believed location and the circle is the agent's true location. The same holds for the red distribution which represents the agent's belief of the location of an object. \textbf {b)} Evolution of the covariance components of $\Sigma $ over time and true $Y_t$ and expected measurements, $\hat {Y}_t$. $\Sigma _a$ and $\Sigma _o$ are the variances of the agent and object positions and $\Sigma _{ao}$ is the cross-covariance term.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Top}: \textit  {Left:} Initialisation of the agent and object joint distribution. \textit  {Right:} Marginals of the agent and object parameterised by $\boldsymbol  {\theta }_a$ and $\boldsymbol  {\theta }_o$, giving the probability of their location. The marginal of each random variable is obtained from Equation \ref  {eq:agent_marginal}. The probability of the agent and object being in state $s=6$ is given by summing the blue and red highlighted parameters in the joint distribution. \textbf  {Bottom}: 1D world Likelihood $P(Y_t|A_t,O)$, the white regions $A \cap O$ will leave the joint distribution unchanged whilst the black regions will evaluate the joint distribution to zero. \textit  {Left:} No contact detected with the object, the current measurement is $Y_t = \leavevmode {\color {red}0}$, both the agent and object cannot be in the same state. \textit  {Right:} The agent entered into contact with the object and received a haptic feedback $Y_t = \leavevmode {\color {red}1}$. The agent receives only two measurement possibilities, contact or no contact. \relax }}{8}{figure.caption.4}}
\newlabel{fig:histogram_joint}{{4}{8}{\textbf {Top}: \textit {Left:} Initialisation of the agent and object joint distribution. \textit {Right:} Marginals of the agent and object parameterised by $\ThA $ and $\ThO $, giving the probability of their location. The marginal of each random variable is obtained from Equation \ref {eq:agent_marginal}. The probability of the agent and object being in state $s=6$ is given by summing the blue and red highlighted parameters in the joint distribution. \textbf {Bottom}: 1D world Likelihood $P(Y_t|A_t,O)$, the white regions $A \cap O$ will leave the joint distribution unchanged whilst the black regions will evaluate the joint distribution to zero. \textit {Left:} No contact detected with the object, the current measurement is $Y_t = \textcolor {red}0$, both the agent and object cannot be in the same state. \textit {Right:} The agent entered into contact with the object and received a haptic feedback $Y_t = \textcolor {red}1$. The agent receives only two measurement possibilities, contact or no contact. \relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{2.2}}Histogram-SLAM}{8}{subsection.2.2}}
\newlabel{sec:Discrete}{{{2.2}}{8}{Histogram-SLAM}{subsection.2.2}{}}
\newlabel{eq:agent_marginal}{{4}{8}{Histogram-SLAM}{equation.2.4}{}}
\newlabel{eq:discrete_likelihoood}{{5}{9}{Histogram-SLAM}{equation.2.5}{}}
\newlabel{eq:ch5:disc_prod_AO}{{6}{9}{Histogram-SLAM}{equation.2.6}{}}
\newlabel{eq:disc_motion}{{7}{9}{Histogram-SLAM}{equation.2.7}{}}
\newlabel{eq:disc_measurement}{{8}{9}{Histogram-SLAM}{equation.2.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Histogram-SLAM, 4 time steps. \textbf  {1} Application of likelihood $P(Y_0=0|A_0,O)$ and the agent remains stationary, all states along the green line become zero. \textbf  {2} The agent moves to the right $u_1=1$, the motion $P(A_1|A_0,u_1)$, and likelihood models are applied consecutively. The right motion results in a shift (black arrow on the left) in the joint probability distribution towards the state $i=10$. All parameters on the pink line are zero. \textbf  {3} Same as two. At each time step a new likelihood function (pink line) is applied to the joint distribution.\relax }}{10}{figure.caption.5}}
\newlabel{fig:discrete_example}{{5}{10}{Histogram-SLAM, 4 time steps. \textbf {1} Application of likelihood $P(Y_0=0|A_0,O)$ and the agent remains stationary, all states along the green line become zero. \textbf {2} The agent moves to the right $u_1=1$, the motion $P(A_1|A_0,u_1)$, and likelihood models are applied consecutively. The right motion results in a shift (black arrow on the left) in the joint probability distribution towards the state $i=10$. All parameters on the pink line are zero. \textbf {3} Same as two. At each time step a new likelihood function (pink line) is applied to the joint distribution.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Measurement Likelihood Memory Filter}{11}{section.3}}
\newlabel{sec:MLMF}{{3}{11}{Measurement Likelihood Memory Filter}{section.3}{}}
\newlabel{eq:prod_AO}{{9}{11}{Measurement Likelihood Memory Filter}{equation.3.9}{}}
\newlabel{eq:mlmf_motion_update}{{10}{11}{Measurement Likelihood Memory Filter}{equation.3.10}{}}
\newlabel{eq:mlmf_measurement_update}{{11}{11}{Measurement Likelihood Memory Filter}{equation.3.11}{}}
\newlabel{eq:mlmf_filter_conditional}{{12}{11}{Measurement Likelihood Memory Filter}{equation.3.12}{}}
\newlabel{eq:marignal_mrf}{{13}{11}{Measurement Likelihood Memory Filter}{equation.3.13}{}}
\newlabel{eq:memory}{{15}{11}{Measurement Likelihood Memory Filter}{equation.3.15}{}}
\newlabel{eq:ch5:liklihood_v2}{{16}{12}{Measurement Likelihood Memory Filter}{equation.3.16}{}}
\newlabel{eq:offset}{{17}{12}{Measurement Likelihood Memory Filter}{equation.3.17}{}}
\newlabel{alg:memory-motion}{{1}{12}{Measurement Likelihood Memory Filter}{algocfline.1}{}}
\newlabel{alg:motion_memory}{{1}{12}{Measurement Likelihood Memory Filter}{algocfline.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Memory Likelihood update\relax }}{12}{algocf.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Un-normalised MLMF joint distribution, numerator of Equation \ref  {eq:mlmf_filter_conditional}, at time $t=3$. Three measurements (all $Y=0$) and two actions (both $u=1$) have been integrated into the joint distribution, for simplicity we do not consider any motion noise. \textit  {Left column:} The first plot illustrates the likelihood of the first measurement $Y_0$. We highlight the contour in light-green to indicate that it was the first applied likelihood function (see the correspondence with Figure \ref  {fig:discrete_example}). The first likelihood function has been moved by the 2 actions, the second likelihood function has been moved by one action (the last one, $u_2=1$) and the third likelihood has had no action applied to it yet. The last applied likelihood function is highlighted in pink and the product of all the likelihoods since $t=0$ until $t=3$ is depicted at the bottom of the figure which is $P(Y_{0:2}|A_2,O,u_{1:2})$. \textit  {Right column:} the top figure illustrates the original marginal of the object $P(O;\boldsymbol  {\theta }^*_o)$, which remains unchanged, and the agent's marginal $P(A_2|u_{1:2};\boldsymbol  {\theta }^*_a)$ which has moved in accordance to the motion update function. Their product would results in the joint distribution $P(A_2,O|u_{1:2};\boldsymbol  {\theta }^*_a,\boldsymbol  {\theta }^*_o)$ illustrated in the middle figure if evaluated at each state $(i,j)$. The bottom figure is the result of multiplying $P(A_2,O|u_{1:2};\boldsymbol  {\theta }^*_a,\boldsymbol  {\theta }^*_o)$ with $P(Y_{0:2}|A_2,O,u_{1:2};\boldsymbol  {\Psi _{0:2}})$ giving the filtered joint distribution, Equation \ref  {eq:mlmf_filter_conditional}. \relax }}{13}{figure.caption.6}}
\newlabel{fig:maringal_joint_example_v2}{{6}{13}{Un-normalised MLMF joint distribution, numerator of Equation \ref {eq:mlmf_filter_conditional}, at time $t=3$. Three measurements (all $Y=0$) and two actions (both $u=1$) have been integrated into the joint distribution, for simplicity we do not consider any motion noise. \textit {Left column:} The first plot illustrates the likelihood of the first measurement $Y_0$. We highlight the contour in light-green to indicate that it was the first applied likelihood function (see the correspondence with Figure \ref {fig:discrete_example}). The first likelihood function has been moved by the 2 actions, the second likelihood function has been moved by one action (the last one, $u_2=1$) and the third likelihood has had no action applied to it yet. The last applied likelihood function is highlighted in pink and the product of all the likelihoods since $t=0$ until $t=3$ is depicted at the bottom of the figure which is $P(Y_{0:2}|A_2,O,u_{1:2})$. \textit {Right column:} the top figure illustrates the original marginal of the object $P(O;\ThOs )$, which remains unchanged, and the agent's marginal $P(A_2|u_{1:2};\ThAs )$ which has moved in accordance to the motion update function. Their product would results in the joint distribution $P(A_2,O|u_{1:2};\ThAs ,\ThOs )$ illustrated in the middle figure if evaluated at each state $(i,j)$. The bottom figure is the result of multiplying $P(A_2,O|u_{1:2};\ThAs ,\ThOs )$ with $P(Y_{0:2}|A_2,O,u_{1:2};\boldsymbol {\Psi _{0:2}})$ giving the filtered joint distribution, Equation \ref {eq:mlmf_filter_conditional}. \relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{3.1}}Evidence and marginals}{14}{subsection.3.1}}
\newlabel{eq:joint_independent_dependent}{{18}{14}{Evidence and marginals}{equation.3.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {{{3.1}.1}}Evidence}{14}{subsubsection.3.1.1}}
\newlabel{eq:I_v1}{{19}{14}{Evidence}{equation.3.19}{}}
\newlabel{eq:I_v2}{{20}{14}{Evidence}{equation.3.20}{}}
\citation{PF_tutorial_2002}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {{{3.1}.2}}Marginals}{15}{subsubsection.3.1.2}}
\newlabel{eq:marignal_mrf}{{21}{15}{Marginals}{equation.3.21}{}}
\newlabel{eq:marignal_mrf_2}{{22}{15}{Marginals}{equation.3.22}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces MLMF functions with associated parameters. The marginal parameters are the discretisation of the state space $\boldsymbol  {\theta } \in \mathbb  {R}^N$, $\boldsymbol  {\theta }^{(s)}$ correspond to the probability being in state $s$.\relax }}{15}{table.caption.8}}
\newlabel{tab:mlmf_parameters}{{1}{15}{MLMF functions with associated parameters. The marginal parameters are the discretisation of the state space $\boldsymbol {\theta } \in \mathbb {R}^N$, $\boldsymbol {\theta }^{(s)}$ correspond to the probability being in state $s$.\relax }{table.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Filtered marginals. Illustration of the agent and object marginal update, Equation \ref  {eq:marignal_mrf}. The joint distribution parameters which are independent $A \ominus O$ are pale and the dependent areas $A \cap O$, where $P(Y_t<1|A_t,O)$, are bright. MLMF only evaluates the joint distribution in dependent states. For each state $s$ of the marginals $1,\dots  ,10$ the difference of the marginals inside the dependent area, before and after the measurement likelihood is applied, is evaluated and removed from the marginals $P(A_t|Y_{0:t-1},u_{1:t};\boldsymbol  {\theta }_a)$, $P(O|Y_{0:t-1};\boldsymbol  {\theta }_o)$ leading to $P(A_t|Y_{0:t},u_{1:t};\boldsymbol  {\theta }_a)$, $P(O|Y_{0:t};\boldsymbol  {\theta }_o)$ (we did not show $u_{1:t}$ in the figure for ease of notation). \textit  {Bottom-left}: joint marginals $P(A_t|u_{1:t};\boldsymbol  {\theta }^*_a)$ and $P(O;\boldsymbol  {\theta }^*_o)$ remain unchanged by measurements.\relax }}{16}{figure.caption.7}}
\newlabel{fig:marginal_update}{{7}{16}{Filtered marginals. Illustration of the agent and object marginal update, Equation \ref {eq:marignal_mrf}. The joint distribution parameters which are independent $A \ominus O$ are pale and the dependent areas $A \cap O$, where $P(Y_t<1|A_t,O)$, are bright. MLMF only evaluates the joint distribution in dependent states. For each state $s$ of the marginals $1,\dots ,10$ the difference of the marginals inside the dependent area, before and after the measurement likelihood is applied, is evaluated and removed from the marginals $P(A_t|Y_{0:t-1},u_{1:t};\ThA )$, $P(O|Y_{0:t-1};\ThO )$ leading to $P(A_t|Y_{0:t},u_{1:t};\ThA )$, $P(O|Y_{0:t};\ThO )$ (we did not show $u_{1:t}$ in the figure for ease of notation). \textit {Bottom-left}: joint marginals $P(A_t|u_{1:t};\ThAs )$ and $P(O;\ThOs )$ remain unchanged by measurements.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Space \& time complexity}{16}{section.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \textit  {Left:} \textbf  {Joint distribution} $P(A,O^{(1)},O^{(2)})$ of the agent and two objects ($Y_{0:t}$ and $u_{1:t}$ omitted). Each likelihood function, $P(Y|A,O^{(1)})$, $P(Y|A,O^{(2)})$ corresponds to a hyperplane in the joint distribution The state space is discretised to $N$ bins giving a potential total of $N^3$ parameters for the joint distribution (Histogram case). \textit  {Right:} \textbf  {Scalable-MLMF} Each agent-object joint distribution pair is modelled independently. For clarity we have left out the action random variable $u$ which is linked to every agent node. Two joint distributions $P(A^{(1)},O^{(1)}|Y^{(1)}_{0:t})$ and $P(A^{(2)},O^{(2)}|Y^{(2)}_{0:t})$ parametrise the graphical model. The dashed undirected lines represent a wanted dependency, if present $O^{(1)}$ and $O^{(2)}$ are to be dependent through $A$. In the standard setting there will be no exchange of information between the individual joint distributions. However we demonstrate later on how we perform a one time transfer of information when one of the objects is sensed.\relax }}{17}{figure.caption.9}}
\newlabel{fig:3bel_lik_profile}{{8}{17}{\textit {Left:} \textbf {Joint distribution} $P(A,O^{(1)},O^{(2)})$ of the agent and two objects ($Y_{0:t}$ and $u_{1:t}$ omitted). Each likelihood function, $P(Y|A,O^{(1)})$, $P(Y|A,O^{(2)})$ corresponds to a hyperplane in the joint distribution The state space is discretised to $N$ bins giving a potential total of $N^3$ parameters for the joint distribution (Histogram case). \textit {Right:} \textbf {Scalable-MLMF} Each agent-object joint distribution pair is modelled independently. For clarity we have left out the action random variable $u$ which is linked to every agent node. Two joint distributions $P(A^{(1)},O^{(1)}|Y^{(1)}_{0:t})$ and $P(A^{(2)},O^{(2)}|Y^{(2)}_{0:t})$ parametrise the graphical model. The dashed undirected lines represent a wanted dependency, if present $O^{(1)}$ and $O^{(2)}$ are to be dependent through $A$. In the standard setting there will be no exchange of information between the individual joint distributions. However we demonstrate later on how we perform a one time transfer of information when one of the objects is sensed.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{4.1}}Space complexity}{17}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{4.2}}Time complexity}{17}{subsection.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Scalable extension to multiple objects}{18}{section.5}}
\newlabel{subsec:scalabe_extension}{{5}{18}{Scalable extension to multiple objects}{section.5}{}}
\newlabel{eq:pair_wise_joint}{{23}{18}{Scalable extension to multiple objects}{equation.5.23}{}}
\newlabel{eq:marg_indep}{{24}{18}{Scalable extension to multiple objects}{equation.5.24}{}}
\newlabel{eq:marg_indep_prod}{{25}{18}{Scalable extension to multiple objects}{equation.5.25}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textbf  {Time and space complexity summary} For both MLMF and scalabe-MLMF the worst case scenario is reported for the space complexity.\relax }}{19}{table.caption.12}}
\newlabel{tab:time_space_summary}{{2}{19}{\textbf {Time and space complexity summary} For both MLMF and scalabe-MLMF the worst case scenario is reported for the space complexity.\relax }{table.caption.12}{}}
\newlabel{eq:marg_indep_sum}{{26}{19}{Scalable extension to multiple objects}{equation.5.26}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Evaluation}{19}{section.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces \textbf  {Transfer of information (joint distributions)} \textit  {Top:} Joint distributions of $P(A^{(1)}_t,O^{(1)}|Y^{(1)})$ and $P(A^{(2)}_t,O^{(2)}|Y^{(2)})$ prior sensing, $Y_t^{(2)}=1$, see Figure \ref  {fig:independence_object} (\textit  {Top right}) for the corresponding marginals. The red and green lines across the joint distributions correspond to the region in which the likelihood functions $P(Y^{(1)}_{t}|A^{(1)}_t,O^{(1)})$ and $P(Y^{(2)}_{t}|A^{(2)}_t,O^{(2)})$ will change the joint distributions. The dotted blue lines are to ease the comparison ofthe joint distributions prior and post sensing. \textit  {Bottom right:} After the agent has sensed $O^{(2)}$, all the probability mass which was not overlapping the green line becomes an infeasible solution to the agent and object locations. At this point the marginals $P(A^{(1)}_t|u_{1:t}) \not = P(A^{(2)}_t|u_{1:t})$ are no longer equal (see the blue marginals \textit  {Top}). \textit  {Bottom left:} The constraint imposed by the likelihood function of the second object (green line) is transferred to the joint distribution of the first object according to Algorithm \ref  {alg:scalabe-mrf-slam}. This results in a change in the joint distribution $P(A^{(1)}_t,O^{(1)}|Y^{(1)})$, which satisfies the constraints imposed by the agent's marginal from the joint distribution $P(A^{(2)}_t,O^{(2)}|Y^{(2)})$.\relax }}{20}{figure.caption.10}}
\newlabel{fig:transfer_information}{{9}{20}{\textbf {Transfer of information (joint distributions)} \textit {Top:} Joint distributions of $P(A^{(1)}_t,O^{(1)}|Y^{(1)})$ and $P(A^{(2)}_t,O^{(2)}|Y^{(2)})$ prior sensing, $Y_t^{(2)}=1$, see Figure \ref {fig:independence_object} (\textit {Top right}) for the corresponding marginals. The red and green lines across the joint distributions correspond to the region in which the likelihood functions $P(Y^{(1)}_{t}|A^{(1)}_t,O^{(1)})$ and $P(Y^{(2)}_{t}|A^{(2)}_t,O^{(2)})$ will change the joint distributions. The dotted blue lines are to ease the comparison ofthe joint distributions prior and post sensing. \textit {Bottom right:} After the agent has sensed $O^{(2)}$, all the probability mass which was not overlapping the green line becomes an infeasible solution to the agent and object locations. At this point the marginals $P(A^{(1)}_t|u_{1:t}) \not = P(A^{(2)}_t|u_{1:t})$ are no longer equal (see the blue marginals \textit {Top}). \textit {Bottom left:} The constraint imposed by the likelihood function of the second object (green line) is transferred to the joint distribution of the first object according to Algorithm \ref {alg:scalabe-mrf-slam}. This results in a change in the joint distribution $P(A^{(1)}_t,O^{(1)}|Y^{(1)})$, which satisfies the constraints imposed by the agent's marginal from the joint distribution $P(A^{(2)}_t,O^{(2)}|Y^{(2)})$.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{6.1}}Evaluation of time complexity}{20}{subsection.6.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \textbf  {Transfer of information (marginals)} \textit  {Top left:} Initial beliefs of the agent and object's location. The agent moves to the left until it senses object $O^{(2)}$. \textit  {Top right:} Marginals prior the agent entering in contact with the green object, see Figure \ref  {fig:transfer_information} (\textit  {Top}) for an illustrate of the joint distributions. \textit  {Bottom left:} resulting marginals after setting the agent marginals of each joint distribution equal $A^{(1)}_t = A^{(2)}_t$ according to Algorithm \ref  {alg:scalabe-mrf-slam}. The object marginal $P(O^{(2)}|Y_{0:t})$ is recomputed. \textit  {Bottom Right:} resulting marginals in which the objects have no influence on one another. Note that a transfer of information has caused a change in the marginal $O^{(1)}$.\relax }}{21}{figure.caption.11}}
\newlabel{fig:independence_object}{{10}{21}{\textbf {Transfer of information (marginals)} \textit {Top left:} Initial beliefs of the agent and object's location. The agent moves to the left until it senses object $O^{(2)}$. \textit {Top right:} Marginals prior the agent entering in contact with the green object, see Figure \ref {fig:transfer_information} (\textit {Top}) for an illustrate of the joint distributions. \textit {Bottom left:} resulting marginals after setting the agent marginals of each joint distribution equal $A^{(1)}_t = A^{(2)}_t$ according to Algorithm \ref {alg:scalabe-mrf-slam}. The object marginal $P(O^{(2)}|Y_{0:t})$ is recomputed. \textit {Bottom Right:} resulting marginals in which the objects have no influence on one another. Note that a transfer of information has caused a change in the marginal $O^{(1)}$.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{6.2}}Evaluation of the independence assumption}{21}{subsection.6.2}}
\newlabel{subsec:eval_indep_assumptiom}{{{6.2}}{21}{Evaluation of the independence assumption}{subsection.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces \textbf  {Time complexity:} \textit  {left:} mean time taken for a loop update (motion and measurement) as a function of the number of states in a marginal and the number of objects present. \textit  {right:} time taken for a loop update with respect to the number of states in the marginal. The colour coded lines are associated with the number of objects present. The computational cost is plotted on a log scale. As the number of states increases exponentially the computational cost matches it.\relax }}{22}{figure.caption.13}}
\newlabel{fig:time_complexity}{{11}{22}{\textbf {Time complexity:} \textit {left:} mean time taken for a loop update (motion and measurement) as a function of the number of states in a marginal and the number of objects present. \textit {right:} time taken for a loop update with respect to the number of states in the marginal. The colour coded lines are associated with the number of objects present. The computational cost is plotted on a log scale. As the number of states increases exponentially the computational cost matches it.\relax }{figure.caption.13}{}}
\newlabel{eq:hellinger}{{27}{22}{Evaluation of the independence assumption}{equation.6.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces \textbf  {Comparison of scalable-MLMF and the histogram filter} A deterministic sweep policy was carried out for 100 different initialisations of the agent and object beliefs. \textit  {Top left:} One particular Initialisation of the agent and object random variables. The true position of the agent and objects were sampled at random. The black arrow indicates the general policy which was followed for each of the 100 sweeps. These were performed for \textbf  {1)} scalable-MLMF with objects considered to be independent at all times (no Algorithm \ref  {alg:scalabe-mrf-slam}). \textbf  {2)} Agent marginal $P(A_t|Y_{0:t},u_{1:t})$ is the product of marginals $P(A^{(i)}_t|Y^{(i)}_{0:t},u_{1:t})$, Equation \ref  {eq:marg_indep_prod}. \textbf  {3)} marginal $P(A_t|Y_{0:t},u_{1:t})$ is taken to be the average of all marginals $P(A^{(i)}_t|Y^{(i)}_{0:t},u_{1:t})$, Equation \ref  {eq:marg_indep_sum}. For each of these three experiment we report the kernel density estimation over the Hellinger distances taken at every time step between ground truth (from histogram filter) and scalable-MLMF.\relax }}{23}{figure.caption.14}}
\newlabel{fig:independence_assumption_test}{{12}{23}{\textbf {Comparison of scalable-MLMF and the histogram filter} A deterministic sweep policy was carried out for 100 different initialisations of the agent and object beliefs. \textit {Top left:} One particular Initialisation of the agent and object random variables. The true position of the agent and objects were sampled at random. The black arrow indicates the general policy which was followed for each of the 100 sweeps. These were performed for \textbf {1)} scalable-MLMF with objects considered to be independent at all times (no Algorithm \ref {alg:scalabe-mrf-slam}). \textbf {2)} Agent marginal $P(A_t|Y_{0:t},u_{1:t})$ is the product of marginals $P(A^{(i)}_t|Y^{(i)}_{0:t},u_{1:t})$, Equation \ref {eq:marg_indep_prod}. \textbf {3)} marginal $P(A_t|Y_{0:t},u_{1:t})$ is taken to be the average of all marginals $P(A^{(i)}_t|Y^{(i)}_{0:t},u_{1:t})$, Equation \ref {eq:marg_indep_sum}. For each of these three experiment we report the kernel density estimation over the Hellinger distances taken at every time step between ground truth (from histogram filter) and scalable-MLMF.\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{6.3}}Evaluation of memory}{23}{subsection.6.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces \textbf  {Agent's prior beliefs.} Two types of environment, the first is a 2D world where the agent lives in a square surrounded by a wall whilst the second is a 1D world. In the 2D figures the agent is illustrated by a circle with a bar to indicate its heading. The true location of the objects are represented by colour coded squares. \textit  {Top row} three different initialisations of the agent's location. \textit  {Bottom row} d) the agent's prior beliefs with respect to the location of the first object and e) belief of the second object's location. \textit  {bottom row} f) 1D world with one object.\relax }}{24}{figure.caption.15}}
\newlabel{fig:exploration_init}{{13}{24}{\textbf {Agent's prior beliefs.} Two types of environment, the first is a 2D world where the agent lives in a square surrounded by a wall whilst the second is a 1D world. In the 2D figures the agent is illustrated by a circle with a bar to indicate its heading. The true location of the objects are represented by colour coded squares. \textit {Top row} three different initialisations of the agent's location. \textit {Bottom row} d) the agent's prior beliefs with respect to the location of the first object and e) belief of the second object's location. \textit {bottom row} f) 1D world with one object.\relax }{figure.caption.15}{}}
\newlabel{eq:greedy_algorithm}{{28}{24}{Evaluation of memory}{equation.6.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces \textbf  {Memory size vs time to find object in 1D} Results of the effect of the memory size on the decision process for the 1D search illustrated in Figure \ref  {fig:exploration_init} \textit  {f)}. The memory size is reported as the percentage of total number of states present in the marginal space. At 100\% the size of the memory is equal to that of the state space, $N=100$ in this case. A total sweep of the entire state space would result in a total of 200 steps, the dotted grey line in the above figure. When no restrictions are placed on the memory size the policy following the greedy approach takes around 180 steps. This result converges when the number of parameters $|\Psi _{0:t}|$ of the memory likelihood function is greater than 50\% of the original state space. \relax }}{25}{figure.caption.16}}
\newlabel{fig:time_to_reach_goal_1D}{{14}{25}{\textbf {Memory size vs time to find object in 1D} Results of the effect of the memory size on the decision process for the 1D search illustrated in Figure \ref {fig:exploration_init} \textit {f)}. The memory size is reported as the percentage of total number of states present in the marginal space. At 100\% the size of the memory is equal to that of the state space, $N=100$ in this case. A total sweep of the entire state space would result in a total of 200 steps, the dotted grey line in the above figure. When no restrictions are placed on the memory size the policy following the greedy approach takes around 180 steps. This result converges when the number of parameters $|\Psi _{0:t}|$ of the memory likelihood function is greater than 50\% of the original state space. \relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{25}{section.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces \textbf  {Memory size vs time to find objects in 2D}. The initial beliefs correspond to those of Figure \ref  {fig:exploration_init}, a) for Gaussian (green line), b) 4 Gaussians (red line) and c) Uniform (blue line), both objects are initialised according to d) and e).\relax }}{26}{figure.caption.17}}
\newlabel{fig:time_to_reach_goal_2D}{{15}{26}{\textbf {Memory size vs time to find objects in 2D}. The initial beliefs correspond to those of Figure \ref {fig:exploration_init}, a) for Gaussian (green line), b) 4 Gaussians (red line) and c) Uniform (blue line), both objects are initialised according to d) and e).\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Appendix}{28}{section.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{8.1}}MLMF Algorithm}{28}{subsection.8.1}}
\newlabel{app:alg:mlmf}{{{8.1}}{28}{MLMF Algorithm}{subsection.8.1}{}}
\newlabel{alg:mrf-slam}{{2}{28}{MLMF Algorithm}{algocfline.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces MLMF-SLAM\relax }}{28}{algocf.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{8.2}}Scalabe-MLMF Algorithm}{29}{subsection.8.2}}
\newlabel{app:scalable-mlmf}{{{8.2}}{29}{Scalabe-MLMF Algorithm}{subsection.8.2}{}}
\newlabel{alg:scalabe-mrf-slam}{{3}{29}{Scalabe-MLMF Algorithm}{algocfline.3}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Scalable-MLMF: Measurement Update\relax }}{29}{algocf.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{8.3}}Recursion example}{29}{subsection.8.3}}
\newlabel{appendix:recursion_example}{{{8.3}}{29}{Recursion example}{subsection.8.3}{}}
\newlabel{eq:ch5:rec_ex1}{{32}{29}{Recursion example}{equation.8.32}{}}
\newlabel{eq:ch5:rec_ex2}{{33}{29}{Recursion example}{equation.8.33}{}}
\newlabel{eq:ch5:rec_ex3}{{37}{30}{Recursion example}{equation.8.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{8.4}}Derivation of the evidence}{30}{subsection.8.4}}
\newlabel{appendix:evidence}{{{8.4}}{30}{Derivation of the evidence}{subsection.8.4}{}}
\newlabel{eq:ch5:numerator}{{42}{30}{Derivation of the evidence}{equation.8.42}{}}
\newlabel{eq:ch5:gradient_alpha}{{46}{31}{Derivation of the evidence}{equation.8.46}{}}
\newlabel{eq:ch5:evidence_yo}{{49}{31}{Derivation of the evidence}{equation.8.49}{}}
\newlabel{eq:ch5:evidence_y}{{50}{31}{Derivation of the evidence}{equation.8.50}{}}
\bibstyle{frontiersinSCNS_ENG_HUMS}
\bibdata{bib/MLMF.bib}
\bibcite{PF_tutorial_2002}{{1}{2002}{{Arulampalam et~al.}}{{Arulampalam, Maskell, Gordon, and Clapp}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{8.5}}Derivation of the marginal}{32}{subsection.8.5}}
\newlabel{appendix:marginal}{{{8.5}}{32}{Derivation of the marginal}{subsection.8.5}{}}
\newlabel{eq:ch5:app:dummy}{{51}{32}{Derivation of the marginal}{equation.8.51}{}}
\newlabel{eq:dependence1}{{52}{32}{Derivation of the marginal}{equation.8.52}{}}
\newlabel{eq:dependence2}{{53}{32}{Derivation of the marginal}{equation.8.53}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{8.6}}Figures}{32}{subsection.8.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {{8.7}}Tables}{32}{subsection.8.7}}
\bibcite{Bake_Saxe_Tene_2011}{{2}{2011}{{Bake et~al.}}{{Bake, Tenenbaum, and Saxe}}}
\bibcite{KL_SLAM_exploration_PF}{{3}{2010}{{Carlone et~al.}}{{Carlone, Du, Ng, Bona, and Indri}}}
\bibcite{ActivePosSLAM}{{4}{2012{a}}{{Carrillo et~al.}}{{Carrillo, Reid, and Castellanos}}}
\bibcite{Active_SLAM_Uncertainty_compar}{{5}{2012{b}}{{Carrillo et~al.}}{{Carrillo, Reid, and Castellanos}}}
\bibcite{deChambrier2013}{{6}{2013}{{de~Chambrier and Billard}}{{}}}
\bibcite{SLAM_part1}{{7}{2006}{{Durrant-Whyte and Bailey}}{{}}}
\bibcite{Navigation_strategires_for_exploring_indoor_environments}{{8}{2002}{{Gonz{\'{a}}lez{-}Ba{\~{n}}os and Latombe}}{{}}}
\bibcite{TutGraphSLAM}{{9}{2010}{{Grisetti et~al.}}{{Grisetti, Kummerle, Stachniss, and Burgard}}}
\bibcite{negative_info_markov_localisation}{{10}{2005}{{Hoffman et~al.}}{{Hoffman, Spranger, Gohring, and Jungel}}}
\bibcite{NegInfoFurtherStudies}{{11}{2006}{{Hoffmann et~al.}}{{Hoffmann, Spranger, Gohring, Jungel, and Burkhard}}}
\bibcite{RRT-SLAM}{{12}{2008}{{Huang and Gupta}}{{}}}
\bibcite{PRM_1996}{{13}{1996}{{Kavraki et~al.}}{{Kavraki, Svestka, Latombe, and Overmars}}}
\bibcite{Kollar_2008_Exploration_SLAM}{{14}{2008}{{Kollar and Roy}}{{}}}
\bibcite{GeorgiosLidoris}{{15}{2011}{{Lidoris}}{{}}}
\bibcite{DataAssociation2003}{{16}{2003}{{Montemerlo and Thrun}}{{}}}
\bibcite{FastSLAM}{{17}{2003}{{Montemerlo et~al.}}{{Montemerlo, Thrun, Koller, and Wegbreit}}}
\bibcite{Plagemann07gaussianbeam}{{18}{2007}{{Plagemann et~al.}}{{Plagemann, Kersting, Pfaff, and Burgard}}}
\bibcite{Ross08onlineplanning}{{19}{2008}{{Ross et~al.}}{{Ross, Pineau, Paquet, and Chaib-draa}}}
\bibcite{BayesBall}{{20}{1998}{{Shachter}}{{}}}
\bibcite{stachniss05robotics}{{21}{2005}{{Stachniss et~al.}}{{Stachniss, Grisetti, and Burgard}}}
\bibcite{Thrun02particlefilters}{{22}{2002}{{Thrun}}{{}}}
\bibcite{Thrun_grid_based_1996}{{23}{1996}{{Thrun and B\"{u}}}{{}}}
\bibcite{Thrun_Burgard_Fox_2005}{{24}{2005}{{Thrun et~al.}}{{Thrun, Burgard, and Fox}}}
\bibcite{SLAM_HBR}{{25}{2008}{{Thrun and Leonard}}{{}}}
\global\@namedef{@lastpage@}{34}
